[
  {
    "Model": "Claude 3 Haiku",
    "Description": "Claude 3 Haiku is Anthropic's fastest and most cost-effective language model, designed for rapid response times and efficient processing. With 20B parameters and a 200K token context window, it excels at quick tasks like content summarization, customer support, and real-time applications while maintaining Anthropic's commitment to AI safety.",
    "Meta-description": "Claude 3 Haiku by Anthropic - Fast, cost-effective 20B parameter AI model with 200K context. Perfect for customer support, content summarization, and rapid AI applications. Ranked #21 operationally.",
    "OperationalRank": "#21",
    "SafetyRank": "N/A",
    "Org.": "Anthropic",
    "Size": "20B Parameters",
    "Released": "07-Mar-24",
    "CodeLMArena": "1184",
    "MathLiveBench": "22.90%",
    "CodeLiveBench": "-",
    "Input Cost/M": "$0.25 ",
    "Output Cost/M": "$1.25 ",
    "CutoffKnowledge": "2023-08-01",
    "ContextLength": "200,000 tokens",
    "License": "Proprietary",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Claude 3 Opus",
    "Description": "Claude 3 Opus represents Anthropic's most powerful and sophisticated language model with 2T parameters. Built for complex reasoning, creative tasks, and advanced problem-solving, it delivers exceptional performance across diverse domains while maintaining strong safety measures and ethical AI principles.",
    "Meta-description": "Claude 3 Opus by Anthropic - Most powerful 2T parameter AI model with advanced reasoning capabilities. Ideal for complex analysis, creative writing, and sophisticated problem-solving tasks.",
    "OperationalRank": "#76",
    "SafetyRank": "N/A",
    "Org.": "Anthropic",
    "Size": "2T Parameters",
    "Released": "04-Mar-24",
    "CodeLMArena": "1236",
    "MathLiveBench": "43.40%",
    "CodeLiveBench": "-",
    "Input Cost/M": "$15 ",
    "Output Cost/M": "$75 ",
    "CutoffKnowledge": "2023-08-01",
    "ContextLength": "200,000 tokens",
    "License": "Proprietary",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Claude 3.5 Haiku",
    "Description": "Claude 3.5 Haiku combines speed and intelligence in Anthropic's updated compact model. With improved 20B parameters and enhanced coding capabilities (28% CodeLiveBench), it offers better performance than its predecessor while maintaining rapid response times and cost efficiency for production applications.",
    "Meta-description": "Claude 3.5 Haiku by Anthropic - Enhanced 20B parameter model with improved coding abilities (28% CodeLiveBench). Fast, efficient AI for production applications and development tasks.",
    "OperationalRank": "#26",
    "SafetyRank": "N/A",
    "Org.": "Anthropic",
    "Size": "20B Parameters",
    "Released": "22-Oct-24",
    "CodeLMArena": "1263",
    "MathLiveBench": "35.50%",
    "CodeLiveBench": "28.00%",
    "Input Cost/M": "$1 ",
    "Output Cost/M": "$5 ",
    "CutoffKnowledge": "2024-07-01",
    "ContextLength": "200,000 tokens",
    "License": "Proprietary",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Claude 3.5 Sonnet",
    "Description": "Claude 3.5 Sonnet delivers exceptional performance with 70B parameters, achieving strong results in coding (32.3% CodeLiveBench) and mathematical reasoning (51.3% MathLiveBench). This balanced model excels at complex analysis, software development, and professional writing while maintaining Anthropic's safety standards.",
    "Meta-description": "Claude 3.5 Sonnet by Anthropic - 70B parameter model with excellent coding (32.3%) and math abilities (51.3%). Perfect for software development, analysis, and professional applications.",
    "OperationalRank": "#34",
    "SafetyRank": "N/A",
    "Org.": "Anthropic",
    "Size": "70B Parameters",
    "Released": "22-Oct-24",
    "CodeLMArena": "1313",
    "MathLiveBench": "51.30%",
    "CodeLiveBench": "32.30%",
    "Input Cost/M": "$3 ",
    "Output Cost/M": "$15 ",
    "CutoffKnowledge": "2024-04-01",
    "ContextLength": "200,000 tokens",
    "License": "Proprietary",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Claude 3.7 Sonnet",
    "Description": "Claude 3.7 Sonnet sets the gold standard for AI safety with #1 safety ranking, achieving 100% safe responses and perfect jailbreaking resistance. With enhanced 70B parameters and superior mathematical reasoning (63.3% MathLiveBench), it's the ideal choice for enterprise applications requiring maximum safety and reliability.",
    "Meta-description": "Claude 3.7 Sonnet by Anthropic - #1 safest AI model with 100% safe responses and perfect security. 70B parameters with excellent math skills (63.3%). Enterprise-grade safety and performance.",
    "OperationalRank": "#31",
    "SafetyRank": "#1",
    "Org.": "Anthropic",
    "Size": "70B Parameters",
    "Released": "24-Feb-25",
    "CodeLMArena": "1326",
    "MathLiveBench": "63.30%",
    "CodeLiveBench": "32.40%",
    "Input Cost/M": "$3 ",
    "Output Cost/M": "$15 ",
    "CutoffKnowledge": "2024-10-01",
    "ContextLength": "200,000 tokens",
    "License": "Proprietary",
    "SafeResponses": "100%",
    "UnsafeResponses": "0%",
    "JailbreakingResistance": "100%",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Claude 3.7 Sonnet Thinking",
    "Description": "Claude 3.7 Sonnet Thinking incorporates advanced reasoning capabilities with explicit thinking processes. Featuring 70B parameters and exceptional mathematical performance (77.5% MathLiveBench), this model excels at complex problem-solving, step-by-step analysis, and transparent reasoning for research and educational applications.",
    "Meta-description": "Claude 3.7 Sonnet Thinking by Anthropic - Advanced 70B parameter model with transparent reasoning. Exceptional math performance (77.5%) and step-by-step problem-solving capabilities.",
    "OperationalRank": "#30",
    "SafetyRank": "N/A",
    "Org.": "Anthropic",
    "Size": "70B Parameters",
    "Released": "24-Feb-25",
    "CodeLMArena": "1333",
    "MathLiveBench": "77.50%",
    "CodeLiveBench": "44.70%",
    "Input Cost/M": "$3 ",
    "Output Cost/M": "$15 ",
    "CutoffKnowledge": "2024-10-01",
    "ContextLength": "200,000 tokens",
    "License": "Proprietary",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Codestral 25.01",
    "Description": "Codestral 25.01 is Mistral's latest specialized coding model, designed specifically for software development tasks. With an extended 256K token context window, it handles large codebases and complex programming projects efficiently, making it ideal for developers and software engineering teams.",
    "Meta-description": "Codestral 25.01 by Mistral - Specialized coding AI model with 256K context window. Perfect for software development, large codebase analysis, and programming assistance.",
    "OperationalRank": "#50",
    "SafetyRank": "N/A",
    "Org.": "Mistral",
    "Size": "-",
    "Released": "14-Jan-25",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "-",
    "Output Cost/M": "-",
    "CutoffKnowledge": null,
    "ContextLength": "256,000 tokens",
    "License": "Proprietary",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "DeepSeek 2.5",
    "Description": "DeepSeek 2.5 delivers impressive performance with 236B parameters at an exceptional value proposition. Ranked #9 operationally, it offers strong coding capabilities and cost-effective pricing, making it an attractive option for businesses seeking high performance without premium costs.",
    "Meta-description": "DeepSeek 2.5 - High-performance 236B parameter AI model ranked #9 globally. Cost-effective solution for coding, analysis, and business applications with excellent value.",
    "OperationalRank": "#9",
    "SafetyRank": "N/A",
    "Org.": "DeepSeek",
    "Size": "236B Parameters",
    "Released": "12-Sep-24",
    "CodeLMArena": "1255",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "$0.13 ",
    "Output Cost/M": "$0.38 ",
    "CutoffKnowledge": "2023-11-01",
    "ContextLength": "128,000 tokens",
    "License": "Proprietary",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "DeepSeek Coder 2",
    "Description": "DeepSeek Coder 2 is a specialized programming model with 236B parameters, optimized for software development tasks. It provides excellent coding assistance at competitive pricing, making it ideal for developers, coding bootcamps, and software companies seeking reliable programming AI support.",
    "Meta-description": "DeepSeek Coder 2 - Specialized 236B parameter coding AI model. Excellent programming assistance at competitive prices. Perfect for developers and software development teams.",
    "OperationalRank": "#10",
    "SafetyRank": "N/A",
    "Org.": "DeepSeek",
    "Size": "236B Parameters",
    "Released": "22-Jul-24",
    "CodeLMArena": "1226",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "$0.14 ",
    "Output Cost/M": "$0.28 ",
    "CutoffKnowledge": "2023-11-01",
    "ContextLength": "128,000 tokens",
    "License": "Proprietary",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "DeepSeek R1",
    "Description": "DeepSeek R1 represents a breakthrough in reasoning AI with 671B parameters and exceptional mathematical capabilities (79.5% MathLiveBench). Ranked #5 in safety with strong protective measures (89% safe responses), it combines advanced reasoning with responsible AI deployment for complex analytical tasks.",
    "Meta-description": "DeepSeek R1 - Advanced 671B parameter reasoning AI with exceptional math skills (79.5%) and #5 safety ranking. Perfect for complex analysis and mathematical problem-solving.",
    "OperationalRank": "#14",
    "SafetyRank": "#5",
    "Org.": "DeepSeek",
    "Size": "671B Parameters",
    "Released": "20-Jan-25",
    "CodeLMArena": "-",
    "MathLiveBench": "79.50%",
    "CodeLiveBench": "48.50%",
    "Input Cost/M": "$0.55 ",
    "Output Cost/M": "$2.19 ",
    "CutoffKnowledge": "2024-07-01",
    "ContextLength": "128,000 tokens",
    "License": "Open Source",
    "SafeResponses": "89%",
    "UnsafeResponses": "11%",
    "JailbreakingResistance": "32%",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "DeepSeek V3",
    "Description": "DeepSeek V3 is a massive 685B parameter open-source model that delivers exceptional performance in mathematical reasoning (73.5% MathLiveBench) and coding tasks (40.5% CodeLiveBench). As an open-source solution, it provides enterprise-level capabilities with transparency and customization options.",
    "Meta-description": "DeepSeek V3 - Massive 685B parameter open-source AI model with excellent math (73.5%) and coding (40.5%) performance. Enterprise capabilities with open-source transparency.",
    "OperationalRank": "#13",
    "SafetyRank": "N/A",
    "Org.": "DeepSeek",
    "Size": "685B Parameters",
    "Released": "24-Mar-25",
    "CodeLMArena": "-",
    "MathLiveBench": "73.50%",
    "CodeLiveBench": "40.50%",
    "Input Cost/M": "$0.27 ",
    "Output Cost/M": "$1.10 ",
    "CutoffKnowledge": "2024-07-01",
    "ContextLength": "128,000 tokens",
    "License": "Open Source",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "DeepSeek V3",
    "Description": "DeepSeek V3 (671B variant) offers remarkable value with 671B parameters and strong mathematical reasoning (60.5% MathLiveBench). With extremely competitive pricing and open-source accessibility, it democratizes access to high-performance AI for researchers, developers, and organizations worldwide.",
    "Meta-description": "DeepSeek V3 671B - Cost-effective open-source AI model with strong math performance (60.5%). Democratizing high-performance AI with competitive pricing and transparency.",
    "OperationalRank": "#13",
    "SafetyRank": "N/A",
    "Org.": "DeepSeek",
    "Size": "671B Parameters",
    "Released": "26-Dec-24",
    "CodeLMArena": "1279",
    "MathLiveBench": "60.50%",
    "CodeLiveBench": "-",
    "Input Cost/M": "$0.07 ",
    "Output Cost/M": "$1.10 ",
    "CutoffKnowledge": "2024-07-01",
    "ContextLength": "128,000 tokens",
    "License": "Open Source",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Gemini 1.5 Flash",
    "Description": "Gemini 1.5 Flash is Google's speed-optimized model with 8B parameters, designed for rapid response and high throughput applications. Ranked #2 operationally with ultra-low pricing and massive 1M token context, it's perfect for real-time applications, chatbots, and high-volume processing tasks.",
    "Meta-description": "Gemini 1.5 Flash by Google - Ultra-fast 8B parameter AI model ranked #2 globally. 1M token context with ultra-low pricing. Perfect for real-time applications and chatbots.",
    "OperationalRank": "#2",
    "SafetyRank": "N/A",
    "Org.": "Google",
    "Size": "8B Parameters",
    "Released": "24-Sep-24",
    "CodeLMArena": "1167",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "$0.04 ",
    "Output Cost/M": "$0.15 ",
    "CutoffKnowledge": "2024-09-01",
    "ContextLength": "1M tokens",
    "License": "Proprietary",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Gemini 2.0 Flash",
    "Description": "Gemini 2.0 Flash represents Google's next-generation speed model with enhanced capabilities. Ranked #3 operationally, it delivers excellent mathematical reasoning (65.6% MathLiveBench) and coding performance (26.2% CodeLiveBench) while maintaining rapid response times and competitive pricing.",
    "Meta-description": "Gemini 2.0 Flash by Google - Next-gen speed model ranked #3 with excellent math (65.6%) and coding abilities. Enhanced performance with rapid response times.",
    "OperationalRank": "#3",
    "SafetyRank": "N/A",
    "Org.": "Google",
    "Size": "-",
    "Released": "05-Feb-25",
    "CodeLMArena": "1271",
    "MathLiveBench": "65.60%",
    "CodeLiveBench": "26.20%",
    "Input Cost/M": "$0.10 ",
    "Output Cost/M": "$0.40 ",
    "CutoffKnowledge": "2024-08-01",
    "ContextLength": "1M tokens",
    "License": "Proprietary",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Gemini 2.0 Flash-Lite",
    "Description": "Gemini 2.0 Flash-Lite offers a streamlined version of Google's advanced model, balancing performance with efficiency. With strong mathematical capabilities (55.5% MathLiveBench) and cost-effective pricing, it's ideal for applications requiring good performance without premium costs.",
    "Meta-description": "Gemini 2.0 Flash-Lite by Google - Streamlined AI model with strong math performance (55.5%) and cost-effective pricing. Balanced performance for budget-conscious applications.",
    "OperationalRank": "#5",
    "SafetyRank": "N/A",
    "Org.": "Google",
    "Size": "-",
    "Released": "05-Feb-25",
    "CodeLMArena": "1246",
    "MathLiveBench": "55.50%",
    "CodeLiveBench": "23.40%",
    "Input Cost/M": "$0.08 ",
    "Output Cost/M": "$0.30 ",
    "CutoffKnowledge": "2024-08-01",
    "ContextLength": "1M tokens",
    "License": "Proprietary",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Gemini 2.0 Pro",
    "Description": "Gemini 2.0 Pro is Google's flagship professional model with excellent mathematical reasoning (71% MathLiveBench) and superior coding capabilities (35.3% CodeLiveBench). Featuring a massive 2M token context window, it's designed for complex enterprise applications requiring extended context understanding.",
    "Meta-description": "Gemini 2.0 Pro by Google - Flagship professional AI with excellent math (71%) and coding (35.3%) performance. 2M token context for complex enterprise applications.",
    "OperationalRank": "#18",
    "SafetyRank": "N/A",
    "Org.": "Google",
    "Size": "-",
    "Released": "05-Feb-25",
    "CodeLMArena": "1298",
    "MathLiveBench": "71.00%",
    "CodeLiveBench": "35.30%",
    "Input Cost/M": "-",
    "Output Cost/M": "-",
    "CutoffKnowledge": "2024-08-01",
    "ContextLength": "2M tokens",
    "License": "Proprietary",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Gemini 2.5 Flash",
    "Description": "Gemini 2.5 Flash represents Google's latest speed innovation with exceptional coding performance (58.4% CodeLiveBench) and current knowledge cutoff. This cutting-edge model combines rapid response times with advanced capabilities, making it ideal for modern development and real-time applications.",
    "Meta-description": "Gemini 2.5 Flash by Google - Latest speed model with exceptional coding performance (58.4%). Current knowledge and rapid response for modern development applications.",
    "OperationalRank": "#4",
    "SafetyRank": "N/A",
    "Org.": "Google",
    "Size": "-",
    "Released": "17-Apr-25",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "58.40%",
    "Input Cost/M": "$0.15 ",
    "Output Cost/M": "$0.60 ",
    "CutoffKnowledge": "2025-01-01",
    "ContextLength": "1M tokens",
    "License": "Proprietary",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Gemini 2.5 Pro",
    "Description": "Gemini 2.5 Pro sets new standards with exceptional mathematical reasoning (90.2% MathLiveBench) and superior coding performance (58.1% CodeLiveBench). Ranked #6 globally, this flagship model delivers state-of-the-art performance across all domains with current knowledge and enterprise-grade capabilities.",
    "Meta-description": "Gemini 2.5 Pro by Google - State-of-the-art AI ranked #6 with exceptional math (90.2%) and coding (58.1%) performance. Enterprise-grade capabilities with current knowledge.",
    "OperationalRank": "#6",
    "SafetyRank": "N/A",
    "Org.": "Google",
    "Size": "-",
    "Released": "25-Mar-25",
    "CodeLMArena": "1352",
    "MathLiveBench": "90.20%",
    "CodeLiveBench": "58.10%",
    "Input Cost/M": "$1.25 ",
    "Output Cost/M": "$10 ",
    "CutoffKnowledge": "2025-01-01",
    "ContextLength": "1M tokens",
    "License": "Proprietary",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Gemini Exp 1114",
    "Description": "Gemini Exp 1114 is Google's experimental model showcasing advanced research capabilities with strong mathematical performance (54.9% MathLiveBench). As an experimental release, it provides early access to cutting-edge AI research and innovative approaches to language understanding.",
    "Meta-description": "Gemini Exp 1114 by Google - Experimental AI model with advanced research capabilities and strong math performance (54.9%). Early access to cutting-edge AI innovation.",
    "OperationalRank": "#43",
    "SafetyRank": "N/A",
    "Org.": "Google",
    "Size": "-",
    "Released": "14-Nov-24",
    "CodeLMArena": "-",
    "MathLiveBench": "54.90%",
    "CodeLiveBench": "-",
    "Input Cost/M": "-",
    "Output Cost/M": "-",
    "CutoffKnowledge": "2024-08-01",
    "ContextLength": "32,000 tokens",
    "License": "Proprietary",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Gemini Exp 1121",
    "Description": "Gemini Exp 1121 builds upon Google's experimental framework with enhanced mathematical reasoning (62.7% MathLiveBench). This research model explores advanced AI capabilities and serves as a testbed for innovative features that may appear in future production releases.",
    "Meta-description": "Gemini Exp 1121 by Google - Enhanced experimental AI model with improved math reasoning (62.7%). Research testbed for future AI innovations and advanced capabilities.",
    "OperationalRank": "#37",
    "SafetyRank": "N/A",
    "Org.": "Google",
    "Size": "-",
    "Released": "21-Nov-24",
    "CodeLMArena": "-",
    "MathLiveBench": "62.70%",
    "CodeLiveBench": "-",
    "Input Cost/M": "-",
    "Output Cost/M": "-",
    "CutoffKnowledge": "2024-08-01",
    "ContextLength": "32,000 tokens",
    "License": "Proprietary",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Gemini Exp 1206",
    "Description": "Gemini Exp 1206 represents Google's latest experimental advancement with strong mathematical performance (70% MathLiveBench) and an impressive 2M token context window. This research model pushes the boundaries of language understanding and context processing for next-generation applications.",
    "Meta-description": "Gemini Exp 1206 by Google - Latest experimental AI with strong math performance (70%) and 2M token context. Pushing boundaries of language understanding and context processing.",
    "OperationalRank": "#20",
    "SafetyRank": "N/A",
    "Org.": "Google",
    "Size": "-",
    "Released": "06-Dec-24",
    "CodeLMArena": "-",
    "MathLiveBench": "70.00%",
    "CodeLiveBench": "-",
    "Input Cost/M": "-",
    "Output Cost/M": "-",
    "CutoffKnowledge": "2024-08-01",
    "ContextLength": "2M tokens",
    "License": "Proprietary",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Gemma 1 2B",
    "Description": "Gemma 1 2B is Google's compact open-source model designed for edge computing and resource-constrained environments. With 2B parameters and coding capabilities (22% CodeLiveBench), it enables on-device AI applications while maintaining Google's quality standards in a lightweight package.",
    "Meta-description": "Gemma 1 2B by Google - Compact 2B parameter open-source model for edge computing. Lightweight AI with coding capabilities for resource-constrained environments.",
    "OperationalRank": "#49",
    "SafetyRank": "N/A",
    "Org.": "Google",
    "Size": "2B Parameters",
    "Released": "21-Feb-24",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "22.00%",
    "Input Cost/M": "-",
    "Output Cost/M": "-",
    "CutoffKnowledge": "2023-07-01",
    "ContextLength": "8,192 tokens",
    "License": "Open Source",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Gemma 1 7B",
    "Description": "Gemma 1 7B offers a balanced approach with 7B parameters, delivering solid mathematical reasoning (51.8% MathLiveBench) and coding performance (32.3% CodeLiveBench). As an open-source model, it provides enterprise-quality capabilities with transparency and customization flexibility.",
    "Meta-description": "Gemma 1 7B by Google - Balanced 7B parameter open-source model with solid math (51.8%) and coding (32.3%) performance. Enterprise-quality with customization flexibility.",
    "OperationalRank": "#48",
    "SafetyRank": "N/A",
    "Org.": "Google",
    "Size": "7B Parameters",
    "Released": "21-Feb-24",
    "CodeLMArena": "-",
    "MathLiveBench": "51.80%",
    "CodeLiveBench": "32.30%",
    "Input Cost/M": "-",
    "Output Cost/M": "-",
    "CutoffKnowledge": "2023-07-01",
    "ContextLength": "8,192 tokens",
    "License": "Open Source",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Gemma 2 27B",
    "Description": "Gemma 2 27B represents Google's powerful open-source offering with 27B parameters, achieving excellent mathematical reasoning (74% MathLiveBench) and superior coding capabilities (51.8% CodeLiveBench). It combines large-scale performance with open-source accessibility for research and enterprise applications.",
    "Meta-description": "Gemma 2 27B by Google - Powerful 27B parameter open-source model with excellent math (74%) and coding (51.8%) performance. Large-scale capabilities with open-source accessibility.",
    "OperationalRank": "#35",
    "SafetyRank": "N/A",
    "Org.": "Google",
    "Size": "27B Parameters",
    "Released": "27-Jun-24",
    "CodeLMArena": "1218",
    "MathLiveBench": "74.00%",
    "CodeLiveBench": "51.80%",
    "Input Cost/M": "-",
    "Output Cost/M": "-",
    "CutoffKnowledge": "2024-04-01",
    "ContextLength": "8,192 tokens",
    "License": "Open Source",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Gemma 2 2B",
    "Description": "Gemma 2 2B is Google's ultra-efficient model designed for mobile and edge applications. With 2B parameters and basic coding capabilities (20.1% CodeLiveBench), it enables AI integration in resource-limited environments while maintaining Google's quality and safety standards.",
    "Meta-description": "Gemma 2 2B by Google - Ultra-efficient 2B parameter model for mobile and edge applications. AI integration for resource-limited environments with quality standards.",
    "OperationalRank": "#51",
    "SafetyRank": "N/A",
    "Org.": "Google",
    "Size": "2B Parameters",
    "Released": "31-Jul-24",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "20.10%",
    "Input Cost/M": "-",
    "Output Cost/M": "-",
    "CutoffKnowledge": "2024-04-01",
    "ContextLength": "8,192 tokens",
    "License": "Open Source",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Gemma 2 9B",
    "Description": "Gemma 2 9B excels with strong mathematical reasoning (68.6% MathLiveBench) and excellent safety measures, ranking #7 in safety with 98% safe responses. This 9B parameter open-source model is ideal for applications requiring both performance and responsible AI deployment.",
    "Meta-description": "Gemma 2 9B by Google - Excellent 9B parameter model with strong math (68.6%) and #7 safety ranking (98% safe). Perfect balance of performance and responsible AI.",
    "OperationalRank": "#39",
    "SafetyRank": "#7",
    "Org.": "Google",
    "Size": "9B Parameters",
    "Released": "27-Jun-24",
    "CodeLMArena": "1187",
    "MathLiveBench": "68.60%",
    "CodeLiveBench": "40.20%",
    "Input Cost/M": "-",
    "Output Cost/M": "-",
    "CutoffKnowledge": "2024-04-01",
    "ContextLength": "8,192 tokens",
    "License": "Open Source",
    "SafeResponses": "98%",
    "UnsafeResponses": "1%",
    "JailbreakingResistance": "2%",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Gemma 3 12B",
    "Description": "Gemma 3 12B represents Google's latest generation open-source model with 12B parameters and an extended 128K token context window. With current knowledge cutoff (2024-12-01), it provides modern capabilities for research, development, and enterprise applications requiring recent information.",
    "Meta-description": "Gemma 3 12B by Google - Latest generation 12B parameter open-source model with 128K context and current knowledge (Dec 2024). Modern capabilities for research and enterprise.",
    "OperationalRank": "#56",
    "SafetyRank": "N/A",
    "Org.": "Google",
    "Size": "12B Parameters",
    "Released": "10-Mar-25",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "-",
    "Output Cost/M": "-",
    "CutoffKnowledge": "2024-12-01",
    "ContextLength": "128,000 tokens",
    "License": "Open Source",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Gemma 3 1B",
    "Description": "Gemma 3 1B is Google's most compact third-generation model with 1B parameters, designed for ultra-lightweight applications. With 128K context window and current knowledge, it enables AI functionality in extremely resource-constrained environments without sacrificing modern capabilities.",
    "Meta-description": "Gemma 3 1B by Google - Ultra-compact 1B parameter model with 128K context and current knowledge. AI functionality for extremely resource-constrained environments.",
    "OperationalRank": "#54",
    "SafetyRank": "N/A",
    "Org.": "Google",
    "Size": "1B Parameters",
    "Released": "10-Mar-25",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "-",
    "Output Cost/M": "-",
    "CutoffKnowledge": "2024-12-01",
    "ContextLength": "128,000 tokens",
    "License": "Open Source",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Gemma 3 27B",
    "Description": "Gemma 3 27B delivers high-performance capabilities with 27B parameters and an enhanced 128K token context window. This latest-generation open-source model provides enterprise-level performance with current knowledge and extended context understanding for complex applications.",
    "Meta-description": "Gemma 3 27B by Google - High-performance 27B parameter open-source model with 128K context and current knowledge. Enterprise-level capabilities for complex applications.",
    "OperationalRank": "#29",
    "SafetyRank": "N/A",
    "Org.": "Google",
    "Size": "27B Parameters",
    "Released": "10-Mar-25",
    "CodeLMArena": "1218",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "-",
    "Output Cost/M": "-",
    "CutoffKnowledge": "2024-12-01",
    "ContextLength": "128,000 tokens",
    "License": "Open Source",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Gemma 3 4B",
    "Description": "Gemma 3 4B is Google's latest-generation model with 4B parameters, designed for ultra-lightweight applications. With 128K context window and current knowledge, it enables AI functionality in extremely resource-constrained environments without sacrificing modern capabilities.",
    "Meta-description": "Gemma 3 4B by Google - Ultra-compact 4B parameter model with 128K context and current knowledge. AI functionality for extremely resource-constrained environments.",
    "OperationalRank": "#55",
    "SafetyRank": "N/A",
    "Org.": "Google",
    "Size": "4B Parameters",
    "Released": "10-Mar-25",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "-",
    "Output Cost/M": "-",
    "CutoffKnowledge": "2024-12-01",
    "ContextLength": "128,000 tokens",
    "License": "Open Source",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "GPT 4.1",
    "Description": "GPT 4.1 represents OpenAI's latest flagship model with exceptional performance across all domains. Ranked #7 operationally and #3 in safety, it delivers outstanding coding (54.6% CodeLiveBench) and mathematical reasoning (72% MathLiveBench) while maintaining strong safety measures with 97% safe responses.",
    "Meta-description": "GPT 4.1 by OpenAI - Latest flagship AI ranked #7 globally with excellent coding (54.6%) and math (72%) performance. #3 safety ranking with 97% safe responses for enterprise use.",
    "OperationalRank": "#7",
    "SafetyRank": "#3",
    "Org.": "OpenAI",
    "Size": "-",
    "Released": "14-Apr-25",
    "CodeLMArena": "1385",
    "MathLiveBench": "72.00%",
    "CodeLiveBench": "54.60%",
    "Input Cost/M": "$2 ",
    "Output Cost/M": "$8 ",
    "CutoffKnowledge": "2024-06-01",
    "ContextLength": "1M tokens",
    "License": "Proprietary",
    "SafeResponses": "97%",
    "UnsafeResponses": "2%",
    "JailbreakingResistance": "34%",
    "Latency": "~0.4s",
    "GPQA": "66.30%"
  },
  {
    "Model": "GPT 4.1 mini",
    "Description": "GPT 4.1 mini offers OpenAI's advanced capabilities in a more cost-effective package. Ranked #8 operationally with strong coding performance (47.6% CodeLiveBench) and 1M token context, it provides excellent value for applications requiring GPT-4 level intelligence at reduced costs.",
    "Meta-description": "GPT 4.1 mini by OpenAI - Cost-effective AI ranked #8 with strong coding performance (47.6%) and 1M context. GPT-4 level intelligence at reduced costs for value-conscious applications.",
    "OperationalRank": "#8",
    "SafetyRank": "N/A",
    "Org.": "OpenAI",
    "Size": "-",
    "Released": "14-Apr-25",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "47.60%",
    "Input Cost/M": "$0.40 ",
    "Output Cost/M": "$1.60 ",
    "CutoffKnowledge": "2024-06-01",
    "ContextLength": "1M tokens",
    "License": "Proprietary",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "~0.3s",
    "GPQA": "65.00%"
  },
  {
    "Model": "GPT 4.1 nano",
    "Description": "GPT 4.1 nano is OpenAI's ultra-efficient model designed for high-volume applications. Ranked #11 operationally and #4 in safety with 95% safe responses, it provides reliable AI capabilities with excellent cost efficiency and safety measures for production deployments.",
    "Meta-description": "GPT 4.1 nano by OpenAI - Ultra-efficient AI ranked #11 with #4 safety ranking (95% safe). Excellent cost efficiency and reliability for high-volume production deployments.",
    "OperationalRank": "#11",
    "SafetyRank": "#4",
    "Org.": "OpenAI",
    "Size": "-",
    "Released": "14-Apr-25",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "25.30%",
    "Input Cost/M": "$0.10 ",
    "Output Cost/M": "$0.40 ",
    "CutoffKnowledge": "2024-06-01",
    "ContextLength": "1M tokens",
    "License": "Proprietary",
    "SafeResponses": "95%",
    "UnsafeResponses": "4%",
    "JailbreakingResistance": "34%",
    "Latency": "~0.2s",
    "GPQA": "50.30%"
  },
  {
    "Model": "GPT 4.5",
    "Description": "GPT 4.5 represents OpenAI's most advanced and secure model, achieving #2 safety ranking with exceptional 99.6% safe responses and 97.3% jailbreaking resistance. With superior coding performance (76.1% CodeLiveBench), it's designed for the most demanding enterprise applications requiring maximum safety and performance.",
    "Meta-description": "GPT 4.5 by OpenAI - Most advanced and secure AI with #2 safety ranking (99.6% safe responses). Superior coding (76.1%) and maximum security for enterprise applications.",
    "OperationalRank": "#77",
    "SafetyRank": "#2",
    "Org.": "OpenAI",
    "Size": "-",
    "Released": "27-Feb-25",
    "CodeLMArena": "1362",
    "MathLiveBench": "69.30%",
    "CodeLiveBench": "76.10%",
    "Input Cost/M": "$75 ",
    "Output Cost/M": "$150 ",
    "CutoffKnowledge": null,
    "ContextLength": "128,000 tokens",
    "License": "Proprietary",
    "SafeResponses": "99.60%",
    "UnsafeResponses": "0.40%",
    "JailbreakingResistance": "97.30%",
    "Latency": "~0.5s",
    "GPQA": "69.50%"
  },
  {
    "Model": "GPT 4o",
    "Description": "GPT 4o delivers exceptional coding performance with 77.5% CodeLiveBench, making it one of the most capable programming assistants available. Ranked #28 operationally with multimodal capabilities and 128K context, it excels at complex software development and technical tasks.",
    "Meta-description": "GPT 4o by OpenAI - Exceptional coding AI with 77.5% CodeLiveBench performance. Multimodal capabilities and 128K context for complex software development and technical tasks.",
    "OperationalRank": "#28",
    "SafetyRank": "N/A",
    "Org.": "OpenAI",
    "Size": "-",
    "Released": "26-Mar-25",
    "CodeLMArena": "1385",
    "MathLiveBench": "-",
    "CodeLiveBench": "77.50%",
    "Input Cost/M": "$5 ",
    "Output Cost/M": "$15 ",
    "CutoffKnowledge": "2023-10-01",
    "ContextLength": "128,000 tokens",
    "License": "Proprietary",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "~0.6s",
    "GPQA": "46.00%"
  },
  {
    "Model": "GPT 4o mini",
    "Description": "GPT 4o mini provides OpenAI's multimodal capabilities in a cost-effective package. Ranked #22 operationally with solid mathematical reasoning (35.6% MathLiveBench) and coding abilities (25.5% CodeLiveBench), it offers an excellent balance of performance and affordability for diverse applications.",
    "Meta-description": "GPT 4o mini by OpenAI - Cost-effective multimodal AI ranked #22 with solid math (35.6%) and coding (25.5%) performance. Excellent balance of capabilities and affordability.",
    "OperationalRank": "#22",
    "SafetyRank": "N/A",
    "Org.": "OpenAI",
    "Size": "-",
    "Released": "18-Jul-24",
    "CodeLMArena": "1245",
    "MathLiveBench": "35.60%",
    "CodeLiveBench": "25.50%",
    "Input Cost/M": "$0.15 ",
    "Output Cost/M": "$0.60 ",
    "CutoffKnowledge": "2023-10-01",
    "ContextLength": "128,000 tokens",
    "License": "Proprietary",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "~0.5s",
    "GPQA": "40.20%"
  },
  {
    "Model": "Grok-3",
    "Description": "Grok-3 is xAI's large-scale language model with 300B parameters, designed for unrestricted conversations and creative applications. Ranked #11 in safety with unique characteristics, it offers a different approach to AI interaction with minimal content filtering for specialized use cases.",
    "Meta-description": "Grok-3 by xAI - Large 300B parameter AI model with unrestricted conversation capabilities. Unique approach to AI interaction with minimal filtering for specialized applications.",
    "OperationalRank": "#70",
    "SafetyRank": "#11",
    "Org.": "xAI",
    "Size": "300B Parameters",
    "Released": "01-May-24",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "-",
    "Output Cost/M": "-",
    "CutoffKnowledge": null,
    "ContextLength": "128,000 tokens",
    "License": "Proprietary",
    "SafeResponses": "2.70%",
    "UnsafeResponses": "97.30%",
    "JailbreakingResistance": "2.70%",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Llama 3.1",
    "Description": "Llama 3.1 is Meta's powerful open-source model with 70B parameters, offering strong coding capabilities (1209 CodeLMArena) and mathematical reasoning (34.4% MathLiveBench). With 128K context and competitive pricing, it provides enterprise-level performance with open-source flexibility.",
    "Meta-description": "Llama 3.1 by Meta - Powerful 70B parameter open-source AI with strong coding and math abilities. Enterprise performance with open-source flexibility and competitive pricing.",
    "OperationalRank": "#23",
    "SafetyRank": "N/A",
    "Org.": "Meta",
    "Size": "70B Parameters",
    "Released": "23-Jul-24",
    "CodeLMArena": "1209",
    "MathLiveBench": "34.40%",
    "CodeLiveBench": "-",
    "Input Cost/M": "$1 ",
    "Output Cost/M": "$3 ",
    "CutoffKnowledge": "2023-12-01",
    "ContextLength": "128,000 tokens",
    "License": "Open Source",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Llama 3.2",
    "Description": "Llama 3.2 is Meta's compact yet efficient model with 3B parameters, designed for cost-effective AI applications. Ranked #12 operationally with ultra-low pricing and 128K context, it's perfect for applications requiring good performance at minimal cost.",
    "Meta-description": "Llama 3.2 by Meta - Compact 3B parameter model ranked #12 with ultra-low pricing. Cost-effective AI for applications requiring good performance at minimal cost.",
    "OperationalRank": "#12",
    "SafetyRank": "N/A",
    "Org.": "Meta",
    "Size": "3B Parameters",
    "Released": "25-Sep-24",
    "CodeLMArena": "1048",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "$0.08 ",
    "Output Cost/M": "$0.10 ",
    "CutoffKnowledge": "2023-12-01",
    "ContextLength": "128,000 tokens",
    "License": "Open Source",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Llama 3.2 (Vision)",
    "Description": "Llama 3.2 Vision extends Meta's capabilities with multimodal processing, featuring 90B parameters for both text and visual understanding. This open-source model enables applications requiring image analysis, visual reasoning, and multimodal AI interactions.",
    "Meta-description": "Llama 3.2 Vision by Meta - Multimodal 90B parameter open-source AI with text and visual understanding. Perfect for image analysis and visual reasoning applications.",
    "OperationalRank": "#38",
    "SafetyRank": "N/A",
    "Org.": "Meta",
    "Size": "90B Parameters",
    "Released": "25-Sep-24",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "$0.90 ",
    "Output Cost/M": "$0.90 ",
    "CutoffKnowledge": "2023-12-01",
    "ContextLength": "128,000 tokens",
    "License": "Open Source",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Llama 3.3",
    "Description": "Llama 3.3 represents Meta's enhanced 70B parameter model with improved mathematical reasoning (41.1% MathLiveBench) and coding capabilities. With ultra-competitive pricing and open-source accessibility, it offers excellent value for developers and organizations seeking reliable AI performance.",
    "Meta-description": "Llama 3.3 by Meta - Enhanced 70B parameter model with improved math (41.1%) and coding abilities. Ultra-competitive pricing with open-source accessibility.",
    "OperationalRank": "#19",
    "SafetyRank": "N/A",
    "Org.": "Meta",
    "Size": "70B Parameters",
    "Released": "06-Dec-24",
    "CodeLMArena": "1232",
    "MathLiveBench": "41.10%",
    "CodeLiveBench": "-",
    "Input Cost/M": "$0.10 ",
    "Output Cost/M": "$0.40 ",
    "CutoffKnowledge": "2023-12-01",
    "ContextLength": "-",
    "License": "Open Source",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Llama 4 Behemoth",
    "Description": "Llama 4 Behemoth is Meta's massive 2T parameter open-source model, designed for the most demanding AI applications. With cutting-edge architecture and current knowledge, it represents the pinnacle of open-source AI capability for research and enterprise use.",
    "Meta-description": "Llama 4 Behemoth by Meta - Massive 2T parameter open-source AI model for demanding applications. Pinnacle of open-source AI capability for research and enterprise use.",
    "OperationalRank": "#58",
    "SafetyRank": "N/A",
    "Org.": "Meta",
    "Size": "2T Parameters",
    "Released": "05-Apr-25",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "-",
    "Output Cost/M": "-",
    "CutoffKnowledge": "2024-08-01",
    "ContextLength": "-",
    "License": "Open Source",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Llama 4 Maverick",
    "Description": "Llama 4 Maverick achieves the #1 operational ranking with 400B parameters, representing Meta's most advanced open-source model. With exceptional performance and ultra-competitive pricing, it sets new standards for open-source AI accessibility and capability.",
    "Meta-description": "Llama 4 Maverick by Meta - #1 ranked 400B parameter open-source AI model. Most advanced open-source AI with exceptional performance and ultra-competitive pricing.",
    "OperationalRank": "#1",
    "SafetyRank": "N/A",
    "Org.": "Meta",
    "Size": "400B Parameters",
    "Released": "05-Apr-25",
    "CodeLMArena": "1265",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "$0.19 ",
    "Output Cost/M": "$0.49 ",
    "CutoffKnowledge": "2024-08-01",
    "ContextLength": "1M tokens",
    "License": "Open Source",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Llama 4 Maverick 17B 128e Instruct",
    "Description": "Llama 4 Maverick 17B is Meta's efficient MoE model with 17B active parameters from a 400B total architecture. Ranked #9 in safety with 93% safe responses, it combines high performance with responsible AI deployment and cost efficiency.",
    "Meta-description": "Llama 4 Maverick 17B by Meta - Efficient MoE model with 17B active parameters and #9 safety ranking (93% safe). High performance with responsible AI deployment.",
    "OperationalRank": "#33",
    "SafetyRank": "#9",
    "Org.": "Meta",
    "Size": "17B Parameters (400B total with MoE)",
    "Released": "05-Apr-25",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "$0.20 ",
    "Output Cost/M": "$0.60 ",
    "CutoffKnowledge": "2024-08-01",
    "ContextLength": "128,000 tokens",
    "License": "Open Source",
    "SafeResponses": "93%",
    "UnsafeResponses": "6%",
    "JailbreakingResistance": "1%",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Llama 4 Scout",
    "Description": "Llama 4 Scout features 109B parameters with an extraordinary 10M token context window, designed for applications requiring massive context understanding. This open-source model excels at processing large documents, codebases, and extended conversations.",
    "Meta-description": "Llama 4 Scout by Meta - 109B parameter model with extraordinary 10M token context window. Perfect for large document processing and extended conversations.",
    "OperationalRank": "#42",
    "SafetyRank": "N/A",
    "Org.": "Meta",
    "Size": "109B Parameters",
    "Released": "05-Apr-25",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "-",
    "Output Cost/M": "-",
    "CutoffKnowledge": "2024-08-01",
    "ContextLength": "10M tokens",
    "License": "Open Source",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "llama-3.1-8b-instant",
    "Description": "Llama 3.1 8B Instant is Meta's optimized model for rapid response applications with 8B parameters. Ranked #8 in safety with 94% safe responses, it provides reliable AI capabilities with excellent safety measures for production deployments requiring speed.",
    "Meta-description": "Llama 3.1 8B Instant by Meta - Optimized 8B parameter model for rapid response with #8 safety ranking (94% safe). Reliable AI with excellent safety for speed-critical applications.",
    "OperationalRank": "#64",
    "SafetyRank": "#8",
    "Org.": "Meta",
    "Size": "8B Parameters",
    "Released": "23-Jul-24",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "-",
    "Output Cost/M": "-",
    "CutoffKnowledge": "2023-12-01",
    "ContextLength": "128,000 tokens",
    "License": "Open Source",
    "SafeResponses": "94%",
    "UnsafeResponses": "5%",
    "JailbreakingResistance": "1%",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Llama-4-scout-17b-16e-instruct",
    "Description": "Llama 4 Scout 17B is Meta's specialized MoE instruction model with 17B parameters, designed for efficient task completion. Ranked #10 in safety with 91% safe responses, it balances performance with responsible AI deployment for instruction-following applications.",
    "Meta-description": "Llama 4 Scout 17B by Meta - Specialized MoE instruction model with #10 safety ranking (91% safe). Balanced performance with responsible AI for instruction-following tasks.",
    "OperationalRank": "#52",
    "SafetyRank": "#10",
    "Org.": "Meta",
    "Size": "17B Parameters (MoE)",
    "Released": "05-Apr-25",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "-",
    "Output Cost/M": "-",
    "CutoffKnowledge": "2024-08-01",
    "ContextLength": "128,000 tokens",
    "License": "Open Source",
    "SafeResponses": "91%",
    "UnsafeResponses": "8%",
    "JailbreakingResistance": "1%",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Mistral Large",
    "Description": "Mistral Large is a powerful 123B parameter model designed for complex reasoning and professional applications. With strong mathematical capabilities (43.7% MathLiveBench) and coding performance, it offers European AI excellence with competitive pricing for enterprise use.",
    "Meta-description": "Mistral Large - Powerful 123B parameter AI model with strong math (43.7%) and coding performance. European AI excellence with competitive pricing for enterprise applications.",
    "OperationalRank": "#25",
    "SafetyRank": "N/A",
    "Org.": "Mistral",
    "Size": "123B Parameters",
    "Released": "18-Nov-24",
    "CodeLMArena": "1228",
    "MathLiveBench": "32.60%",
    "CodeLiveBench": "-",
    "Input Cost/M": "$2 ",
    "Output Cost/M": "$6 ",
    "CutoffKnowledge": "2024-07-01",
    "ContextLength": "128,000 tokens",
    "License": "Proprietary",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Mistral Large",
    "Description": "Mistral Large (July 2024 variant) delivers enhanced performance with 123B parameters and improved mathematical reasoning (43.7% MathLiveBench). This European AI solution provides strong coding capabilities and professional-grade performance for demanding applications.",
    "Meta-description": "Mistral Large July 2024 - Enhanced 123B parameter AI with improved math reasoning (43.7%). European AI solution with strong coding capabilities for professional applications.",
    "OperationalRank": "#25",
    "SafetyRank": "N/A",
    "Org.": "Mistral",
    "Size": "123B Parameters",
    "Released": "24-Jul-24",
    "CodeLMArena": "1244",
    "MathLiveBench": "43.70%",
    "CodeLiveBench": "-",
    "Input Cost/M": "$2 ",
    "Output Cost/M": "$6 ",
    "CutoffKnowledge": "2024-07-01",
    "ContextLength": "128,000 tokens",
    "License": "Proprietary",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Mistral Medium 3",
    "Description": "Mistral Medium 3 offers balanced performance for mid-tier applications requiring reliable AI capabilities. With competitive pricing and European data sovereignty, it's ideal for businesses seeking dependable AI solutions without premium costs.",
    "Meta-description": "Mistral Medium 3 - Balanced AI model for mid-tier applications with competitive pricing. European data sovereignty and dependable performance for business applications.",
    "OperationalRank": "#36",
    "SafetyRank": "N/A",
    "Org.": "Mistral",
    "Size": "-",
    "Released": "07-May-25",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "$0.40 ",
    "Output Cost/M": "$2 ",
    "CutoffKnowledge": null,
    "ContextLength": "-",
    "License": "-",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Mistral Nemo",
    "Description": "Mistral Nemo is a compact 12B parameter model optimized for efficiency and cost-effectiveness. With mathematical reasoning capabilities (16.9% MathLiveBench) and 128K context, it provides reliable AI performance for applications requiring good capabilities at competitive prices.",
    "Meta-description": "Mistral Nemo - Compact 12B parameter model optimized for efficiency with math capabilities (16.9%). Reliable AI performance at competitive prices for cost-conscious applications.",
    "OperationalRank": "#32",
    "SafetyRank": "N/A",
    "Org.": "Mistral",
    "Size": "12B Parameters",
    "Released": "24-Jul-24",
    "CodeLMArena": "-",
    "MathLiveBench": "16.90%",
    "CodeLiveBench": "-",
    "Input Cost/M": "$0.15 ",
    "Output Cost/M": "$0.15 ",
    "CutoffKnowledge": "2024-04-01",
    "ContextLength": "128,000 tokens",
    "License": "Proprietary",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Mistral Small 3",
    "Description": "Mistral Small 3 is a lightweight 24B parameter model designed for efficient AI applications. With compact context and European AI heritage, it provides reliable performance for applications requiring good capabilities without extensive resource requirements.",
    "Meta-description": "Mistral Small 3 - Lightweight 24B parameter model for efficient AI applications. European AI heritage with reliable performance for resource-conscious deployments.",
    "OperationalRank": "#57",
    "SafetyRank": "N/A",
    "Org.": "Mistral",
    "Size": "24B Parameters",
    "Released": "30-Jan-25",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "-",
    "Output Cost/M": "-",
    "CutoffKnowledge": null,
    "ContextLength": "32,000 tokens",
    "License": "Proprietary",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Mistral Small 3.1",
    "Description": "Mistral Small 3.1 enhances the compact model line with 24B parameters and extended 128K context. As an open-source offering, it provides European AI innovation with transparency and customization options for developers and researchers.",
    "Meta-description": "Mistral Small 3.1 - Enhanced 24B parameter open-source model with 128K context. European AI innovation with transparency and customization for developers.",
    "OperationalRank": "#53",
    "SafetyRank": "N/A",
    "Org.": "Mistral",
    "Size": "24B Parameters",
    "Released": "17-Mar-25",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "-",
    "Output Cost/M": "-",
    "CutoffKnowledge": null,
    "ContextLength": "128,000 tokens",
    "License": "Open Source",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Pixtral Large",
    "Description": "Pixtral Large is Mistral's multimodal model with 123B parameters, combining text and visual understanding capabilities. This advanced model enables applications requiring image analysis, visual reasoning, and multimodal AI interactions with European AI standards.",
    "Meta-description": "Pixtral Large by Mistral - Multimodal 123B parameter AI with text and visual understanding. Advanced image analysis and visual reasoning with European AI standards.",
    "OperationalRank": "#47",
    "SafetyRank": "N/A",
    "Org.": "Mistral",
    "Size": "123B Parameters",
    "Released": "18-Nov-24",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "$2 ",
    "Output Cost/M": "$6 ",
    "CutoffKnowledge": null,
    "ContextLength": "128,000 tokens",
    "License": "Proprietary",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Qwen 2.5",
    "Description": "Qwen 2.5 (7B) is Alibaba's efficient model with strong mathematical reasoning (38.2% MathLiveBench) and current knowledge. With 128K context and open-source accessibility, it provides excellent performance for applications requiring modern AI capabilities at scale.",
    "Meta-description": "Qwen 2.5 7B by Alibaba - Efficient model with strong math reasoning (38.2%) and current knowledge. Open-source accessibility with excellent performance for modern AI applications.",
    "OperationalRank": "#15",
    "SafetyRank": "N/A",
    "Org.": "Alibaba",
    "Size": "7B Parameters",
    "Released": "19-Sep-24",
    "CodeLMArena": "-",
    "MathLiveBench": "38.20%",
    "CodeLiveBench": "-",
    "Input Cost/M": "-",
    "Output Cost/M": "-",
    "CutoffKnowledge": "2024-09-01",
    "ContextLength": "128,000 tokens",
    "License": "Open Source",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Qwen 2.5",
    "Description": "Qwen 2.5 (72B) represents Alibaba's flagship open-source model with exceptional mathematical reasoning (52.4% MathLiveBench) and strong coding capabilities. With competitive pricing and current knowledge, it offers enterprise-level performance with transparency.",
    "Meta-description": "Qwen 2.5 72B by Alibaba - Flagship open-source model with exceptional math (52.4%) and coding capabilities. Enterprise performance with competitive pricing and transparency.",
    "OperationalRank": "#15",
    "SafetyRank": "N/A",
    "Org.": "Alibaba",
    "Size": "72B Parameters",
    "Released": "19-Sep-24",
    "CodeLMArena": "1247",
    "MathLiveBench": "52.40%",
    "CodeLiveBench": "-",
    "Input Cost/M": "$0.38 ",
    "Output Cost/M": "$0.57 ",
    "CutoffKnowledge": "2024-09-01",
    "ContextLength": "128,000 tokens",
    "License": "Open Source",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Qwen 2.5",
    "Description": "Qwen 2.5 (32B) offers balanced performance with 32B parameters, providing strong capabilities for mid-scale applications. With current knowledge and open-source accessibility, it's ideal for organizations seeking reliable AI performance with customization flexibility.",
    "Meta-description": "Qwen 2.5 32B by Alibaba - Balanced 32B parameter model for mid-scale applications. Current knowledge and open-source accessibility with customization flexibility.",
    "OperationalRank": "#15",
    "SafetyRank": "N/A",
    "Org.": "Alibaba",
    "Size": "32B Parameters",
    "Released": "19-Sep-24",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "-",
    "Output Cost/M": "-",
    "CutoffKnowledge": "2024-09-01",
    "ContextLength": "128,000 tokens",
    "License": "Open Source",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Qwen 2.5",
    "Description": "Qwen 2.5 (14B) provides efficient AI capabilities with 14B parameters, designed for applications requiring good performance with moderate resource requirements. With current knowledge and open-source licensing, it offers flexibility for diverse use cases.",
    "Meta-description": "Qwen 2.5 14B by Alibaba - Efficient 14B parameter model for moderate resource requirements. Current knowledge and open-source licensing for diverse applications.",
    "OperationalRank": "#15",
    "SafetyRank": "N/A",
    "Org.": "Alibaba",
    "Size": "14B Parameters",
    "Released": "19-Sep-24",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "-",
    "Output Cost/M": "-",
    "CutoffKnowledge": "2024-09-01",
    "ContextLength": "128,000 tokens",
    "License": "Open Source",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Qwen 2.5",
    "Description": "Qwen 2.5 (1.5B) is Alibaba's compact model designed for edge computing and resource-constrained environments. With 1.5B parameters and current knowledge, it enables AI functionality in lightweight applications while maintaining quality performance.",
    "Meta-description": "Qwen 2.5 1.5B by Alibaba - Compact model for edge computing and resource-constrained environments. Current knowledge with quality performance in lightweight applications.",
    "OperationalRank": "#15",
    "SafetyRank": "N/A",
    "Org.": "Alibaba",
    "Size": "1.5B Parameters",
    "Released": "19-Sep-24",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "-",
    "Output Cost/M": "-",
    "CutoffKnowledge": "2024-09-01",
    "ContextLength": "32,000 tokens",
    "License": "Open Source",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Qwen 2.5",
    "Description": "Qwen 2.5 (3B) offers efficient AI capabilities with 3B parameters, balancing performance and resource efficiency. With current knowledge and open-source accessibility, it's perfect for applications requiring reliable AI functionality without extensive computational requirements.",
    "Meta-description": "Qwen 2.5 3B by Alibaba - Efficient 3B parameter model balancing performance and resource efficiency. Current knowledge and open-source accessibility for reliable AI functionality.",
    "OperationalRank": "#15",
    "SafetyRank": "N/A",
    "Org.": "Alibaba",
    "Size": "3B Parameters",
    "Released": "19-Sep-24",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "-",
    "Output Cost/M": "-",
    "CutoffKnowledge": "2024-09-01",
    "ContextLength": "32,000 tokens",
    "License": "Open Source",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Qwen 2.5 Coder",
    "Description": "Qwen 2.5 Coder (32B) is Alibaba's specialized programming model with strong mathematical reasoning (46% MathLiveBench) and coding capabilities. Designed specifically for software development, it provides excellent programming assistance with current knowledge and competitive pricing.",
    "Meta-description": "Qwen 2.5 Coder 32B by Alibaba - Specialized programming model with strong math (46%) and coding capabilities. Excellent programming assistance with current knowledge.",
    "OperationalRank": "#16",
    "SafetyRank": "N/A",
    "Org.": "Alibaba",
    "Size": "32B Parameters",
    "Released": "12-Nov-24",
    "CodeLMArena": "1227",
    "MathLiveBench": "46.00%",
    "CodeLiveBench": "-",
    "Input Cost/M": "$0.20 ",
    "Output Cost/M": "$0.20 ",
    "CutoffKnowledge": "2024-09-01",
    "ContextLength": "128,000 tokens",
    "License": "Open Source",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Qwen 2.5 Coder",
    "Description": "Qwen 2.5 Coder (14B) provides efficient coding assistance with 14B parameters, optimized for software development tasks. With current knowledge and open-source licensing, it offers reliable programming support for developers and coding teams.",
    "Meta-description": "Qwen 2.5 Coder 14B by Alibaba - Efficient coding assistance with 14B parameters for software development. Current knowledge and open-source licensing for programming support.",
    "OperationalRank": "#16",
    "SafetyRank": "N/A",
    "Org.": "Alibaba",
    "Size": "14B Parameters",
    "Released": "12-Nov-24",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "-",
    "Output Cost/M": "-",
    "CutoffKnowledge": "2024-09-01",
    "ContextLength": "128,000 tokens",
    "License": "Open Source",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Qwen 2.5 Coder",
    "Description": "Qwen 2.5 Coder (7B) offers compact coding assistance with 7B parameters, designed for efficient programming support. With current knowledge and open-source accessibility, it provides reliable coding capabilities for resource-conscious development environments.",
    "Meta-description": "Qwen 2.5 Coder 7B by Alibaba - Compact coding assistance with 7B parameters for efficient programming support. Current knowledge and open-source accessibility.",
    "OperationalRank": "#16",
    "SafetyRank": "N/A",
    "Org.": "Alibaba",
    "Size": "7B Parameters",
    "Released": "12-Nov-24",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "-",
    "Output Cost/M": "-",
    "CutoffKnowledge": "2024-09-01",
    "ContextLength": "128,000 tokens",
    "License": "Open Source",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Qwen 2.5 Coder",
    "OperationalRank": "#16",
    "SafetyRank": "N/A",
    "Org.": "Alibaba",
    "Size": "3B Parameters",
    "Released": "12-Nov-24",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "-",
    "Output Cost/M": "-",
    "CutoffKnowledge": "2024-09-01",
    "ContextLength": "32,000 tokens",
    "License": "Open Source",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Qwen 2.5 Coder",
    "OperationalRank": "#16",
    "SafetyRank": "N/A",
    "Org.": "Alibaba",
    "Size": "1.5B Parameters",
    "Released": "12-Nov-24",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "$0.20 ",
    "Output Cost/M": "$0.60 ",
    "CutoffKnowledge": "2024-09-01",
    "ContextLength": "32,000 tokens",
    "License": "Open Source",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Qwen 2.5 Coder",
    "OperationalRank": "#16",
    "SafetyRank": "N/A",
    "Org.": "Alibaba",
    "Size": "0.5B Parameters",
    "Released": "12-Nov-24",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "-",
    "Output Cost/M": "-",
    "CutoffKnowledge": "2024-09-01",
    "ContextLength": "32,000 tokens",
    "License": "Open Source",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Qwen 2.5 Max",
    "OperationalRank": "#24",
    "SafetyRank": "N/A",
    "Org.": "Alibaba",
    "Size": "-",
    "Released": "28-Jan-25",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "33.80%",
    "Input Cost/M": "$0 ",
    "Output Cost/M": "$0.01 ",
    "CutoffKnowledge": "2024-09-01",
    "ContextLength": "32,000 tokens",
    "License": "Proprietary",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Qwen 2.5 Turbo",
    "OperationalRank": "#45",
    "SafetyRank": "N/A",
    "Org.": "Alibaba",
    "Size": "-",
    "Released": "15-Nov-24",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "-",
    "Output Cost/M": "-",
    "CutoffKnowledge": "2024-09-01",
    "ContextLength": "1M tokens",
    "License": "Proprietary",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "Qwen 2.5VL",
    "OperationalRank": "#59",
    "SafetyRank": "N/A",
    "Org.": "Alibaba",
    "Size": "32B Parameters",
    "Released": "24-Mar-25",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "-",
    "Output Cost/M": "-",
    "CutoffKnowledge": null,
    "ContextLength": "-",
    "License": "Open Source",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "qwen-qwq-32b",
    "OperationalRank": "#66",
    "SafetyRank": "#6",
    "Org.": "Alibaba",
    "Size": "32B Parameters",
    "Released": "12-Oct-24",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "-",
    "Output Cost/M": "-",
    "CutoffKnowledge": "2024-09-01",
    "ContextLength": "128,000 tokens",
    "License": "Open Source",
    "SafeResponses": "94%",
    "UnsafeResponses": "5%",
    "JailbreakingResistance": "12%",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "R1 Distill Llama",
    "OperationalRank": "#44",
    "SafetyRank": "N/A",
    "Org.": "DeepSeek",
    "Size": "70B Parameters",
    "Released": "20-Jan-25",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "46.60%",
    "Input Cost/M": "-",
    "Output Cost/M": "-",
    "CutoffKnowledge": "2023-12-01",
    "ContextLength": "-",
    "License": "Open Source",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "R1 Distill Llama",
    "OperationalRank": "#44",
    "SafetyRank": "N/A",
    "Org.": "DeepSeek",
    "Size": "8B Parameters",
    "Released": "20-Jan-25",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "-",
    "Output Cost/M": "-",
    "CutoffKnowledge": "2024-12-01",
    "ContextLength": "-",
    "License": "Open Source",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "R1 Distill Qwen",
    "OperationalRank": "#40",
    "SafetyRank": "N/A",
    "Org.": "DeepSeek",
    "Size": "32B Parameters",
    "Released": "20-Jan-25",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "52.30%",
    "Input Cost/M": "-",
    "Output Cost/M": "-",
    "CutoffKnowledge": "2024-09-01",
    "ContextLength": "-",
    "License": "Open Source",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "R1 Distill Qwen",
    "OperationalRank": "#40",
    "SafetyRank": "N/A",
    "Org.": "DeepSeek",
    "Size": "14B Parameters",
    "Released": "20-Jan-25",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "-",
    "Output Cost/M": "-",
    "CutoffKnowledge": "2024-09-01",
    "ContextLength": "-",
    "License": "Open Source",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "R1 Distill Qwen",
    "OperationalRank": "#40",
    "SafetyRank": "N/A",
    "Org.": "DeepSeek",
    "Size": "7B Parameters",
    "Released": "20-Jan-25",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "-",
    "Output Cost/M": "-",
    "CutoffKnowledge": "2024-09-01",
    "ContextLength": "-",
    "License": "Open Source",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "R1 Distill Qwen",
    "OperationalRank": "#40",
    "SafetyRank": "N/A",
    "Org.": "DeepSeek",
    "Size": "1.5B Parameters",
    "Released": "20-Jan-25",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "-",
    "Output Cost/M": "-",
    "CutoffKnowledge": "2024-09-01",
    "ContextLength": "-",
    "License": "Open Source",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "R1 Lite Preview",
    "OperationalRank": "#75",
    "SafetyRank": "N/A",
    "Org.": "DeepSeek",
    "Size": "-",
    "Released": "20-Nov-24",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "-",
    "Output Cost/M": "-",
    "CutoffKnowledge": null,
    "ContextLength": "-",
    "License": "Proprietary",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "N/A",
    "GPQA": null
  },
  {
    "Model": "GPT-3.5-Turbo-0125",
    "Description": "GPT-3.5-Turbo-0125 is OpenAI's cost-effective and fast language model designed for high-volume applications. With 120 tokens/s output speed and 0.6s latency, it provides reliable AI capabilities at the lowest cost point, making it ideal for chatbots, content generation, and rapid response applications.",
    "Meta-description": "GPT-3.5-Turbo-0125 by OpenAI - Fast, cost-effective AI with 120 tokens/s speed and ultra-low pricing. Perfect for high-volume chatbots and rapid response applications.",
    "OperationalRank": "#125",
    "SafetyRank": "N/A",
    "Org.": "OpenAI",
    "Size": "-",
    "Released": "25-Jan-24",
    "CodeLMArena": "1150",
    "MathLiveBench": "0%",
    "CodeLiveBench": "48%",
    "Input Cost/M": "$0.50",
    "Output Cost/M": "$1.50",
    "CutoffKnowledge": "2021-09-01",
    "ContextLength": "4,000 tokens",
    "License": "Proprietary",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "~0.6s",
    "GPQA": "30.80%"
  },
  {
    "Model": "GPT-4-0613",
    "Description": "GPT-4-0613 represents OpenAI's original flagship model with strong performance across diverse tasks. With 82-86% MMLU score and 67% HumanEval coding performance, it established the foundation for advanced AI capabilities, though it has slower throughput at 30-40 tokens/s.",
    "Meta-description": "GPT-4-0613 by OpenAI - Original flagship model with 82-86% MMLU and 67% coding performance. Foundation model for advanced AI capabilities.",
    "OperationalRank": "#80",
    "SafetyRank": "N/A",
    "Org.": "OpenAI",
    "Size": "-",
    "Released": "13-Jun-23",
    "CodeLMArena": "1380",
    "MathLiveBench": "13%",
    "CodeLiveBench": "67%",
    "Input Cost/M": "$30.00",
    "Output Cost/M": "$60.00",
    "CutoffKnowledge": "2021-09-01",
    "ContextLength": "8,000 tokens",
    "License": "Proprietary",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "~2-3s",
    "GPQA": "35.70%"
  },
  {
    "Model": "GPT-4-Turbo-2024-04-09",
    "Description": "GPT-4-Turbo-2024-04-09 delivers optimized performance with 80 tokens/s throughput and 128K context window. Achieving top-tier coding performance on LiveCodeBench and improved math reasoning over GPT-4, it offers the best balance of capability and speed for professional applications.",
    "Meta-description": "GPT-4-Turbo-2024-04-09 by OpenAI - Optimized performance with 80 tokens/s and 128K context. Top-tier coding and improved math reasoning for professional applications.",
    "OperationalRank": "#5",
    "SafetyRank": "N/A",
    "Org.": "OpenAI",
    "Size": "-",
    "Released": "09-Apr-24",
    "CodeLMArena": "1400",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "$3.75",
    "Output Cost/M": "$15.00",
    "CutoffKnowledge": "2023-12-01",
    "ContextLength": "128,000 tokens",
    "License": "Proprietary",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "~0.5-1.0s",
    "GPQA": "48.00%"
  },
  {
    "Model": "GPT-4o-2024-05-13",
    "Description": "GPT-4o-2024-05-13 is OpenAI's first multimodal 'Omni' model combining text, vision, and audio capabilities. With 84.2% MMLU performance and comparable speed to GPT-3.5-Turbo at 80 tokens/s, it revolutionizes multimodal AI interactions while maintaining GPT-4 level intelligence.",
    "Meta-description": "GPT-4o-2024-05-13 by OpenAI - First multimodal Omni model with 84.2% MMLU and GPT-3.5 speed. Revolutionary multimodal AI with text, vision, and audio capabilities.",
    "OperationalRank": "#15",
    "SafetyRank": "N/A",
    "Org.": "OpenAI",
    "Size": "-",
    "Released": "13-May-24",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "$5.00",
    "Output Cost/M": "$20.00",
    "CutoffKnowledge": "2023-10-01",
    "ContextLength": "128,000 tokens",
    "License": "Proprietary",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "~0.6s",
    "GPQA": null
  },
  {
    "Model": "GPT-4o-2024-08-06",
    "Description": "GPT-4o-2024-08-06 features a 50% price reduction with 78.9% MMLU performance and maintained multimodal capabilities. With improved vision and non-English language support, it offers excellent value for applications requiring multimodal AI at reduced costs.",
    "Meta-description": "GPT-4o-2024-08-06 by OpenAI - 50% price reduction with 78.9% MMLU and enhanced multimodal capabilities. Excellent value for cost-conscious multimodal applications.",
    "OperationalRank": "#20",
    "SafetyRank": "N/A",
    "Org.": "OpenAI",
    "Size": "-",
    "Released": "06-Aug-24",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "$2.50",
    "Output Cost/M": "$10.00",
    "CutoffKnowledge": "2023-10-01",
    "ContextLength": "128,000 tokens",
    "License": "Proprietary",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "~0.6s",
    "GPQA": null
  },
  {
    "Model": "o1-2024-12-17",
    "Description": "o1-2024-12-17 represents OpenAI's breakthrough in reasoning AI with 92% MMLU performance and exceptional mathematical capabilities (83% on IMO qualifiers). Operating at 89th percentile on Codeforces competitions, this model excels at complex problem-solving through advanced reasoning processes.",
    "Meta-description": "o1-2024-12-17 by OpenAI - Breakthrough reasoning AI with 92% MMLU and 83% math performance. 89th percentile Codeforces performance through advanced reasoning processes.",
    "OperationalRank": "#2",
    "SafetyRank": "N/A",
    "Org.": "OpenAI",
    "Size": "-",
    "Released": "17-Dec-24",
    "CodeLMArena": "-",
    "MathLiveBench": "83%",
    "CodeLiveBench": "89%",
    "Input Cost/M": "$27.00",
    "Output Cost/M": "$54.00",
    "CutoffKnowledge": "2023-10-01",
    "ContextLength": "128,000 tokens",
    "License": "Proprietary",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "~10-20s",
    "GPQA": "78.00%"
  },
  {
    "Model": "o1-mini",
    "Description": "o1-mini delivers OpenAI's reasoning capabilities optimized for speed and cost efficiency. With 250 tokens/s throughput and 75% MMLU performance, it provides 80% cost savings over o1-preview while maintaining strong reasoning abilities for mathematical and coding tasks.",
    "Meta-description": "o1-mini by OpenAI - Optimized reasoning AI with 250 tokens/s speed and 75% MMLU. 80% cost savings with maintained reasoning for math and coding tasks.",
    "OperationalRank": "#16",
    "SafetyRank": "N/A",
    "Org.": "OpenAI",
    "Size": "-",
    "Released": "12-Sep-24",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "$3.00",
    "Output Cost/M": "$12.00",
    "CutoffKnowledge": "2023-10-01",
    "ContextLength": "128,000 tokens",
    "License": "Proprietary",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "~1s",
    "GPQA": "60.00%"
  },
  {
    "Model": "o1-preview",
    "Description": "o1-preview was OpenAI's limited-release reasoning model featuring 92.3% MMLU performance and exceptional problem-solving capabilities. With 147 tokens/s throughput but high 22.8s latency, it demonstrated advanced reasoning for research applications before full o1 release.",
    "Meta-description": "o1-preview by OpenAI - Limited-release reasoning model with 92.3% MMLU and exceptional problem-solving. Advanced reasoning demonstration for research applications.",
    "OperationalRank": "#4",
    "SafetyRank": "N/A",
    "Org.": "OpenAI",
    "Size": "-",
    "Released": "12-Sep-24",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "-",
    "Output Cost/M": "-",
    "CutoffKnowledge": "2023-10-01",
    "ContextLength": "128,000 tokens",
    "License": "Proprietary",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "~22.8s",
    "GPQA": "73.30%"
  },
  {
    "Model": "o1-pro",
    "Description": "o1-pro combines OpenAI's advanced reasoning capabilities with ChatGPT tools and browsing integration. Featuring 92% MMLU performance and advanced reasoning with tools, it's designed for professional applications requiring complex problem-solving with real-time information access.",
    "Meta-description": "o1-pro by OpenAI - Advanced reasoning AI with ChatGPT tools integration. 92% MMLU performance for professional applications requiring complex problem-solving.",
    "OperationalRank": "#3",
    "SafetyRank": "N/A",
    "Org.": "OpenAI",
    "Size": "-",
    "Released": "05-Dec-24",
    "CodeLMArena": "-",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "-",
    "Output Cost/M": "-",
    "CutoffKnowledge": "2024-10-01",
    "ContextLength": "128,000 tokens",
    "License": "Proprietary",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "~20s",
    "GPQA": "79.00%"
  },
  {
    "Model": "o3-2025-04-16",
    "Description": "o3-2025-04-16 achieves tied #1 ranking on LMArena with 1442 Elo score and 87% MMLU performance. With excellent mathematical and multilingual capabilities, it represents OpenAI's latest breakthrough in conversational AI, rivaling the best models available.",
    "Meta-description": "o3-2025-04-16 by OpenAI - Tied #1 LMArena ranking with 1442 Elo and 87% MMLU. Latest breakthrough in conversational AI with excellent math and multilingual capabilities.",
    "OperationalRank": "#1",
    "SafetyRank": "N/A",
    "Org.": "OpenAI",
    "Size": "-",
    "Released": "16-Apr-25",
    "CodeLMArena": "1442",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "$5.00",
    "Output Cost/M": "$10.00",
    "CutoffKnowledge": "2024-10-01",
    "ContextLength": "128,000 tokens",
    "License": "Proprietary",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "~0.5-1s",
    "GPQA": "83.30%"
  },
  {
    "Model": "o3-mini",
    "Description": "o3-mini provides OpenAI's next-generation capabilities in a compact, efficient package. With 1255 Elo score and 80.7% MMLU performance, it offers strong reasoning and coding abilities optimized for speed and cost-effectiveness in production applications.",
    "Meta-description": "o3-mini by OpenAI - Next-generation compact AI with 1255 Elo and 80.7% MMLU. Strong reasoning and coding optimized for speed and cost-effectiveness.",
    "OperationalRank": "#18",
    "SafetyRank": "N/A",
    "Org.": "OpenAI",
    "Size": "-",
    "Released": "16-Apr-25",
    "CodeLMArena": "1255",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "-",
    "Output Cost/M": "-",
    "CutoffKnowledge": "2024-10-01",
    "ContextLength": "128,000 tokens",
    "License": "Proprietary",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "~0.3s",
    "GPQA": "77.20%"
  },
  {
    "Model": "o4-mini",
    "Description": "o4-mini represents OpenAI's latest advancement in efficient AI models with 1208 Elo ranking and enhanced vision task capabilities. Designed for next-generation applications, it offers improved reasoning efficiency while maintaining compact size for high-volume deployments.",
    "Meta-description": "o4-mini by OpenAI - Latest efficient AI with 1208 Elo and enhanced vision capabilities. Next-generation model with improved reasoning efficiency for high-volume deployments.",
    "OperationalRank": "#24",
    "SafetyRank": "N/A",
    "Org.": "OpenAI",
    "Size": "-",
    "Released": "01-May-25",
    "CodeLMArena": "1208",
    "MathLiveBench": "-",
    "CodeLiveBench": "-",
    "Input Cost/M": "-",
    "Output Cost/M": "-",
    "CutoffKnowledge": "2024-12-01",
    "ContextLength": "128,000 tokens",
    "License": "Proprietary",
    "SafeResponses": "N/A",
    "UnsafeResponses": "N/A",
    "JailbreakingResistance": "N/A",
    "Latency": "~0.3s",
    "GPQA": "81.40%"
  }
]
