[
    {
      "Model": "GPT-4.1",
      "ModelId": "gpt-4.1-2025-04-14",
      "Description": "GPT 4.1 represents OpenAI's latest flagship model with exceptional performance across all domains. Ranked #7 operationally and #3 in safety, it delivers outstanding coding (54.6% CodeLiveBench) and mathematical reasoning (72% MathLiveBench) while maintaining strong safety measures with 97% safe responses.",
      "Meta-description": "GPT 4.1 by OpenAI - Latest flagship AI ranked #7 globally with excellent coding (54.6%) and math (72%) performance. #3 safety ranking with 97% safe responses for enterprise use.",
      "OperationalRank": "#7",
      "APIReference": "https://platform.openai.com/docs/api-reference",
      "Playground": "https://platform.openai.com/playground",
      "Documentation": "https://platform.openai.com/docs",
      "SafetyRank": "#5",
      "Org.": "OpenAI",
      "Size": "-",
      "Released": "14-Apr-25",
      "CodeLMArena": "1385",
      "MathLiveBench": "72.00%",
      "CodeLiveBench": "54.60%",
      "Input Cost/M": "$2 ",
      "Output Cost/M": "$8 ",
      "CutoffKnowledge": "2024-06-01",
      "ContextLength": "1,047,576 tokens",
      "MaxOutputTokens": "32,768",
      "License": "Proprietary",
      "SafeResponses": "97%",
      "SafeResponsesTotalUsed": 300,
      "UnsafeResponses": "2%",
      "UnsafeResponsesTotalUsed": 300,
      "JailbreakingResistance": "34%",
      "JailbreakingTotalUsed": 37,
      "Latency": "~0.4s",
      "GPQA": "66.30%",
      "Multimodal": "Yes",
      "Reasoning": "No",
      "ProviderModelIds": [
        "gpt-4.1-2025-04-14"
      ],
      "Modalities": {
        "text": {
          "input": true,
          "output": true
        },
        "image": {
          "input": true,
          "output": false
        },
        "audio": {
          "input": false,
          "output": false
        }
      },
      "Features": {
        "streaming": true,
        "function_calling": true,
        "structured_outputs": true,
        "fine_tuning": true,
        "distillation": true,
        "predicted_outputs": true,
        "multimodal": true,
        "reasoning": false
      },
      "Tools": {
        "web_search": true,
        "file_search": true,
        "image_generation": true,
        "code_interpreter": true,
        "mcp": true,
        "computer_use": false
      },
      "providers": {
        "OpenAI": {
          "model_id": "gpt-4.1-2025-04-14",
          "price_per_input_token": 0.000002,
          "price_per_output_token": 0.000008,
          "throughput": 100,
          "latency": 0.4,
          "updated_at": "2025-04-14"
        },
        "Azure": {
          "model_id": "gpt-4.1-2025-04-14",
          "price_per_input_token": 0.000002,
          "price_per_cached_input_token": 5e-7,
          "price_per_output_token": 0.000008,
          "throughput": 95,
          "latency": 0.5,
          "updated_at": "2025-04-14"
        }
      }
    },
    {
      "Model": "GPT-4.1-mini",
      "ModelId": "gpt-4.1-mini-2025-04-14",
      "Description": "GPT‑4.1 mini provides a balance between intelligence, speed, and cost. Supports text+image input, text output—ideal for many use cases.",
      "Meta-description": "Balanced for intelligence, speed, and cost with multimodal input and extensive 1 M token context.",
      "OperationalRank": "N/A",
      "APIReference": "https://platform.openai.com/docs/api-reference",
      "Playground": "https://platform.openai.com/playground",
      "Documentation": "https://platform.openai.com/docs",
      "SafetyRank": "N/A",
      "Org.": "OpenAI",
      "Size": "-",
      "Released": "14-Apr-25",
      "CodeLMArena": "N/A",
      "MathLiveBench": "N/A",
      "CodeLiveBench": "N/A",
      "Input Cost/M": "$0.40",
      "Output Cost/M": "$1.60",
      "CutoffKnowledge": "01-Jun-2024",
      "ContextLength": "1,047,576 tokens",
      "MaxOutputTokens": "32,768",
      "License": "Proprietary",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "Fast (~faster than flagship)",
      "GPQA": "N/A",
      "Multimodal": "Yes",
      "Reasoning": "No",
      "Modalities": {
        "text": {
          "input": true,
          "output": true
        },
        "image": {
          "input": true,
          "output": false
        },
        "audio": {
          "input": false,
          "output": false
        }
      },
      "Features": {
        "streaming": true,
        "function_calling": true,
        "structured_outputs": true,
        "fine_tuning": true,
        "distillation": false,
        "predicted_outputs": false,
        "multimodal": true,
        "reasoning": false
      },
      "Tools": {
        "web_search": true,
        "file_search": true,
        "image_generation": false,
        "code_interpreter": true,
        "mcp": true,
        "computer_use": false
      },
      "Web Access": "Yes (via web search tool)",
      "Fine-tunable": "Yes",
      "ProviderModelIds": [
        "gpt-4.1-mini-2025-04-14"
      ],
      "providers": {
        "OpenAI": {
          "model_id": "gpt-4.1-mini-2025-04-14",
          "price_per_input_token": 4e-7,
          "price_per_output_token": 0.0000016,
          "throughput": "N/A",
          "latency": "Fast",
          "updated_at": "2025-04-14"
        },
        "Azure": {
          "model_id": "gpt-4.1-mini-2025-04-14",
          "price_per_input_token": 4e-7,
          "price_per_cached_input_token": 1e-7,
          "price_per_output_token": 0.0000016,
          "throughput": "N/A",
          "latency": "Fast",
          "updated_at": "2025-04-14"
        }
      }
    },
    {
      "Model": "GPT-4.1-nano",
      "ModelId": "gpt-4.1-nano-2025-04-14",
      "Description": "GPT 4.1 nano is OpenAI's ultra-efficient model designed for high-volume applications. Ranked #11 operationally and #4 in safety with 95% safe responses, it provides reliable AI capabilities with excellent cost efficiency and safety measures for production deployments.",
      "Meta-description": "GPT 4.1 nano by OpenAI - Ultra-efficient AI ranked #11 with #4 safety ranking (95% safe). Excellent cost efficiency and reliability for high-volume production deployments.",
      "OperationalRank": "#11",
      "APIReference": "https://platform.openai.com/docs/api-reference",
      "Playground": "https://platform.openai.com/playground",
      "Documentation": "https://platform.openai.com/docs",
      "SafetyRank": "#6",
      "Org.": "OpenAI",
      "Size": "-",
      "Released": "14-Apr-25",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "25.30%",
      "Input Cost/M": "$0.10 ",
      "Output Cost/M": "$0.40 ",
      "CutoffKnowledge": "2024-06-01",
      "ContextLength": "1M tokens",
      "MaxOutputTokens": "16,384",
      "License": "Proprietary",
      "SafeResponses": "95%",
      "SafeResponsesTotalUsed": 300,
      "UnsafeResponses": "4%",
      "UnsafeResponsesTotalUsed": 300,
      "JailbreakingResistance": "34%",
      "JailbreakingTotalUsed": 37,
      "Latency": "~0.2s",
      "GPQA": "50.30%",
      "Multimodal": "Yes",
      "Reasoning": "No",
      "ProviderModelIds": [
        "gpt-4.1-nano-2025-04-14"
      ],
      "Modalities": {
        "text": {
          "input": true,
          "output": true
        },
        "image": {
          "input": true,
          "output": false
        },
        "audio": {
          "input": false,
          "output": false
        }
      },
      "Features": {
        "streaming": true,
        "function_calling": true,
        "structured_outputs": true,
        "fine_tuning": false,
        "distillation": false,
        "predicted_outputs": false,
        "multimodal": true,
        "reasoning": false
      },
      "Tools": {
        "web_search": false,
        "file_search": true,
        "image_generation": false,
        "code_interpreter": true,
        "mcp": false,
        "computer_use": false
      },
      "Web Access": "No",
      "Fine-tunable": "No",
      "providers": {
        "OpenAI": {
          "model_id": "gpt-4.1-nano-2025-04-14",
          "price_per_input_token": 1e-7,
          "price_per_output_token": 4e-7,
          "throughput": 100,
          "latency": 0.2,
          "updated_at": "2025-04-14"
        },
        "Azure": {
          "model_id": "gpt-4.1-nano-2025-04-14",
          "price_per_input_token": 1e-7,
          "price_per_cached_input_token": 2.5e-8,
          "price_per_output_token": 4e-7,
          "throughput": 95,
          "latency": 0.3,
          "updated_at": "2025-04-14"
        }
      }
    },
    {
      "Model": "GPT-4.5",
      "ModelId": "gpt-4.5-preview-2025-02-27",
      "Description": "GPT 4.5 represents OpenAI's most advanced and secure model, achieving #2 safety ranking with exceptional 99.6% safe responses and 97.3% jailbreaking resistance. With superior coding performance (76.1% CodeLiveBench), it's designed for the most demanding enterprise applications requiring maximum safety and performance.",
      "Meta-description": "GPT 4.5 by OpenAI - Most advanced and secure AI with #2 safety ranking (99.6% safe responses). Superior coding (76.1%) and maximum security for enterprise applications.",
      "OperationalRank": "#77",
      "APIReference": "https://platform.openai.com/docs/api-reference",
      "Playground": "https://platform.openai.com/playground",
      "Documentation": "https://platform.openai.com/docs",
      "SafetyRank": "#2",
      "Org.": "OpenAI",
      "Size": "-",
      "Released": "27-Feb-25",
      "CodeLMArena": "1362",
      "MathLiveBench": "69.30%",
      "CodeLiveBench": "76.10%",
      "Input Cost/M": "$75 ",
      "Output Cost/M": "$150 ",
      "CutoffKnowledge": "2023-10-01",
      "ContextLength": "128,000 tokens",
      "MaxOutputTokens": "16,384",
      "License": "Proprietary",
      "SafeResponses": "99.60%",
      "SafeResponsesTotalUsed": 300,
      "UnsafeResponses": "0.40%",
      "UnsafeResponsesTotalUsed": 300,
      "JailbreakingResistance": "97.30%",
      "JailbreakingTotalUsed": 37,
      "Latency": "~0.5s",
      "GPQA": "69.50%",
      "Multimodal": "Yes",
      "Reasoning": "No",
      "ProviderModelIds": [
        "gpt-4.5-preview-2025-02-27"
      ],
      "Modalities": {
        "text": {
          "input": true,
          "output": true
        },
        "image": {
          "input": true,
          "output": false
        },
        "audio": {
          "input": false,
          "output": false
        }
      },
      "Features": {
        "streaming": true,
        "function_calling": true,
        "structured_outputs": true,
        "fine_tuning": false,
        "distillation": false,
        "predicted_outputs": false,
        "multimodal": true,
        "reasoning": false
      },
      "Tools": {
        "web_search": false,
        "file_search": false,
        "image_generation": false,
        "code_interpreter": true,
        "mcp": false,
        "computer_use": false
      },
      "Web Access": "No",
      "Fine-tunable": "No",
      "providers": {
        "OpenAI": {
          "model_id": "gpt-4.5-preview-2025-02-27",
          "price_per_input_token": 0.000075,
          "price_per_output_token": 0.00015,
          "throughput": 100,
          "latency": 0.5,
          "updated_at": "2025-02-27"
        }
      }
    },
    {
      "Model": "GPT-4o",
      "ModelId": "gpt-4o-2024-11-20",
      "Description": "GPT 4o delivers exceptional coding performance with 77.5% CodeLiveBench, making it one of the most capable programming assistants available. Ranked #28 operationally with multimodal capabilities and 128K context, it excels at complex software development and technical tasks.",
      "Meta-description": "GPT 4o by OpenAI - Exceptional coding AI with 77.5% CodeLiveBench performance. Multimodal capabilities and 128K context for complex software development and technical tasks.",
      "OperationalRank": "#28",
      "APIReference": "https://platform.openai.com/docs/api-reference",
      "Playground": "https://platform.openai.com/playground",
      "Documentation": "https://platform.openai.com/docs",
      "SafetyRank": "#4",
      "Org.": "OpenAI",
      "Size": "-",
      "Released": "20-Nov-24",
      "CodeLMArena": "1385",
      "MathLiveBench": "-",
      "CodeLiveBench": "77.50%",
      "Input Cost/M": "$2.50",
      "Output Cost/M": "$10.00",
      "CutoffKnowledge": "2023-10-01",
      "ContextLength": "128,000 tokens",
      "MaxOutputTokens": "16,384",
      "License": "Proprietary",
      "SafeResponses": "97%",
      "SafeResponsesTotalUsed": 99,
      "UnsafeResponses": "3%",
      "UnsafeResponsesTotalUsed": 99,
      "JailbreakingResistance": "82%",
      "JailbreakingTotalUsed": 100,
      "Latency": "~0.6s",
      "GPQA": "46.00%",
      "Multimodal": "Yes",
      "Reasoning": "No",
      "ProviderModelIds": [
        "gpt-4o-2024-11-20"
      ],
      "Modalities": {
        "text": {
          "input": true,
          "output": true
        },
        "image": {
          "input": true,
          "output": false
        },
        "audio": {
          "input": false,
          "output": false
        }
      },
      "Features": {
        "streaming": true,
        "function_calling": true,
        "structured_outputs": true,
        "fine_tuning": false,
        "distillation": false,
        "predicted_outputs": false,
        "multimodal": true,
        "reasoning": false
      },
      "Tools": {
        "web_search": false,
        "file_search": true,
        "image_generation": false,
        "code_interpreter": true,
        "mcp": false,
        "computer_use": false
      },
      "Web Access": "No",
      "Fine-tunable": "No",
      "providers": {
        "OpenAI": {
          "model_id": "gpt-4o-2024-11-20",
          "price_per_input_token": 0.0000025,
          "price_per_output_token": 0.00001,
          "throughput": 132,
          "latency": 0.6,
          "updated_at": "2024-11-20"
        },
        "Azure": {
          "model_id": "gpt-4o-2024-11-20",
          "price_per_input_token": 0.0000025,
          "price_per_cached_input_token": 0.00000125,
          "price_per_output_token": 0.00001,
          "throughput": 99,
          "latency": 0.53,
          "updated_at": "2024-11-20"
        }
      }
    },
    {
      "Model": "GPT-4o-mini",
      "ModelId": "gpt-4o-mini-2024-07-18",
      "Description": "GPT 4o mini provides OpenAI's multimodal capabilities in a cost-effective package. Ranked #22 operationally with solid mathematical reasoning (35.6% MathLiveBench) and coding abilities (25.5% CodeLiveBench), it offers an excellent balance of performance and affordability for diverse applications.",
      "Meta-description": "GPT 4o mini by OpenAI - Cost-effective multimodal AI ranked #22 with solid math (35.6%) and coding (25.5%) performance. Excellent balance of capabilities and affordability.",
      "OperationalRank": "#22",
      "APIReference": "https://platform.openai.com/docs/api-reference",
      "Playground": "https://platform.openai.com/playground",
      "Documentation": "https://platform.openai.com/docs",
      "SafetyRank": "N/A",
      "Org.": "OpenAI",
      "Size": "-",
      "Released": "18-Jul-24",
      "CodeLMArena": "1245",
      "MathLiveBench": "35.60%",
      "CodeLiveBench": "25.50%",
      "Input Cost/M": "$0.15 ",
      "Output Cost/M": "$0.60 ",
      "CutoffKnowledge": "2023-10-01",
      "ContextLength": "128,000 tokens",
      "MaxOutputTokens": "16,384",
      "License": "Proprietary",
      "SafeResponses": "N/A",
      "SafeResponsesTotalUsed": null,
      "UnsafeResponses": "N/A",
      "UnsafeResponsesTotalUsed": null,
      "JailbreakingResistance": "N/A",
      "JailbreakingTotalUsed": null,
      "Latency": "~0.5s",
      "GPQA": "40.20%",
      "Multimodal": "Yes",
      "Reasoning": "No",
      "ProviderModelIds": [
        "gpt-4o-mini-2024-07-18"
      ],
      "Modalities": {
        "text": {
          "input": true,
          "output": true
        },
        "image": {
          "input": true,
          "output": false
        },
        "audio": {
          "input": false,
          "output": false
        }
      },
      "Features": {
        "streaming": true,
        "function_calling": true,
        "structured_outputs": true,
        "fine_tuning": true,
        "distillation": false,
        "predicted_outputs": false,
        "multimodal": true,
        "reasoning": false
      },
      "Tools": {
        "web_search": false,
        "file_search": true,
        "image_generation": false,
        "code_interpreter": true,
        "mcp": false,
        "computer_use": false
      },
      "Web Access": "No",
      "Fine-tunable": "Yes",
      "providers": {
        "OpenAI": {
          "model_id": "gpt-4o-mini-2024-07-18",
          "price_per_input_token": 1.5e-7,
          "price_per_output_token": 6e-7,
          "throughput": 132,
          "latency": 0.5,
          "updated_at": "2024-07-18"
        },
        "Azure": {
          "model_id": "gpt-4o-mini-2024-07-18",
          "price_per_input_token": 1.65e-7,
          "price_per_output_token": 6.6e-7,
          "throughput": 99,
          "latency": 0.53,
          "updated_at": "2024-07-18"
        }
      }
    },
    {
      "Model": "GPT-3.5-Turbo-0125",
      "Description": "GPT-3.5-Turbo-0125 is OpenAI's cost-effective and fast language model designed for high-volume applications. With 120 tokens/s output speed and 0.6s latency, it provides reliable AI capabilities at the lowest cost point, making it ideal for chatbots, content generation, and rapid response applications.",
      "Meta-description": "GPT-3.5-Turbo-0125 by OpenAI - Fast, cost-effective AI with 120 tokens/s speed and ultra-low pricing. Perfect for high-volume chatbots and rapid response applications.",
      "OperationalRank": "#125",
      "APIReference": "https://platform.openai.com/docs/api-reference",
      "Playground": "https://platform.openai.com/playground",
      "Documentation": "https://platform.openai.com/docs",
      "SafetyRank": "N/A",
      "Org.": "OpenAI",
      "Size": "-",
      "Released": "25-Jan-24",
      "CodeLMArena": "1150",
      "MathLiveBench": "0%",
      "CodeLiveBench": "48%",
      "Input Cost/M": "$0.50",
      "Output Cost/M": "$1.50",
      "CutoffKnowledge": "2021-09-01",
      "ContextLength": "4,000 tokens",
      "License": "Proprietary",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "~0.6s",
      "GPQA": "30.80%",
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "Yes",
      "ProviderModelIds": [
        "gpt-3.5-turbo-0125"
      ],
      "providers": {
        "OpenAI": {
          "model_id": "gpt-3.5-turbo-0125",
          "price_per_input_token": 5e-7,
          "price_per_output_token": 0.0000015,
          "throughput": 100,
          "latency": 0.5,
          "updated_at": "2024-09-09"
        },
        "Azure": {
          "model_id": "gpt-3.5-turbo-0125",
          "price_per_input_token": 5e-7,
          "price_per_output_token": 0.0000015,
          "throughput": 90,
          "latency": 0.8,
          "updated_at": "2024-09-09"
        }
      }
    },
    {
      "Model": "GPT-4-0613",
      "Description": "GPT-4-0613 represents OpenAI's original flagship model with strong performance across diverse tasks. With 82-86% MMLU score and 67% HumanEval coding performance, it established the foundation for advanced AI capabilities, though it has slower throughput at 30-40 tokens/s.",
      "Meta-description": "GPT-4-0613 by OpenAI - Original flagship model with 82-86% MMLU and 67% coding performance. Foundation model for advanced AI capabilities.",
      "OperationalRank": "#80",
      "APIReference": "https://platform.openai.com/docs/api-reference",
      "Playground": "https://platform.openai.com/playground",
      "Documentation": "https://platform.openai.com/docs",
      "SafetyRank": "N/A",
      "Org.": "OpenAI",
      "Size": "-",
      "Released": "13-Jun-23",
      "CodeLMArena": "1380",
      "MathLiveBench": "13%",
      "CodeLiveBench": "67%",
      "Input Cost/M": "$30.00",
      "Output Cost/M": "$60.00",
      "CutoffKnowledge": "2021-09-01",
      "ContextLength": "8,000 tokens",
      "License": "Proprietary",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "~2-3s",
      "GPQA": "35.70%",
      "Multimodal": "Yes",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "Yes",
      "ProviderModelIds": [
        "gpt-4-0613"
      ],
      "providers": {
        "OpenAI": {
          "model_id": "gpt-4-0613",
          "price_per_input_token": 0.00003,
          "price_per_output_token": 0.00006,
          "throughput": 100,
          "latency": 0.5,
          "updated_at": "2024-09-09"
        },
        "Azure": {
          "model_id": "gpt-4-0613",
          "price_per_input_token": 0.00003,
          "price_per_output_token": 0.00006,
          "throughput": 104,
          "latency": 0.3,
          "updated_at": "2024-09-09"
        }
      }
    },
    {
      "Model": "GPT-4-Turbo-2024-04-09",
      "Description": "GPT-4-Turbo-2024-04-09 delivers optimized performance with 80 tokens/s throughput and 128K context window. Achieving top-tier coding performance on LiveCodeBench and improved math reasoning over GPT-4, it offers the best balance of capability and speed for professional applications.",
      "Meta-description": "GPT-4-Turbo-2024-04-09 by OpenAI - Optimized performance with 80 tokens/s and 128K context. Top-tier coding and improved math reasoning for professional applications.",
      "OperationalRank": "#5",
      "APIReference": "https://platform.openai.com/docs/api-reference",
      "Playground": "https://platform.openai.com/playground",
      "Documentation": "https://platform.openai.com/docs",
      "SafetyRank": "N/A",
      "Org.": "OpenAI",
      "Size": "-",
      "Released": "09-Apr-24",
      "CodeLMArena": "1400",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "$3.75",
      "Output Cost/M": "$15.00",
      "CutoffKnowledge": "2023-12-01",
      "ContextLength": "128,000 tokens",
      "License": "Proprietary",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "~0.5-1.0s",
      "GPQA": "48.00%",
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "No",
      "ProviderModelIds": [
        "gpt-4-turbo-2024-04-09"
      ],
      "providers": {
        "OpenAI": {
          "model_id": "gpt-4-turbo-2024-04-09",
          "price_per_input_token": 0.00001,
          "price_per_output_token": 0.00003,
          "throughput": 100,
          "latency": 0.5,
          "updated_at": "2024-09-09"
        },
        "Azure": {
          "model_id": "gpt-4-turbo-2024-04-09",
          "price_per_input_token": 0.00001,
          "price_per_output_token": 0.00003,
          "throughput": 97,
          "latency": 0.6,
          "updated_at": "2024-09-09"
        }
      }
    },
    {
      "Model": "GPT-4o-2024-05-13",
      "Description": "GPT-4o-2024-05-13 is OpenAI's first multimodal 'Omni' model combining text, vision, and audio capabilities. With 84.2% MMLU performance and comparable speed to GPT-3.5-Turbo at 80 tokens/s, it revolutionizes multimodal AI interactions while maintaining GPT-4 level intelligence.",
      "Meta-description": "GPT-4o-2024-05-13 by OpenAI - First multimodal Omni model with 84.2% MMLU and GPT-3.5 speed. Revolutionary multimodal AI with text, vision, and audio capabilities.",
      "OperationalRank": "#15",
      "APIReference": "https://platform.openai.com/docs/api-reference",
      "Playground": "https://platform.openai.com/playground",
      "Documentation": "https://platform.openai.com/docs",
      "SafetyRank": "N/A",
      "Org.": "OpenAI",
      "Size": "-",
      "Released": "13-May-24",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "$5.00",
      "Output Cost/M": "$20.00",
      "CutoffKnowledge": "2023-10-01",
      "ContextLength": "128,000 tokens",
      "License": "Proprietary",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "~0.6s",
      "GPQA": null,
      "Multimodal": "Yes",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "No",
      "ProviderModelIds": [
        "gpt-4o-2024-05-13"
      ],
      "providers": {
        "OpenAI": {
          "model_id": "gpt-4o-2024-05-13",
          "price_per_input_token": 0.0000025,
          "price_per_output_token": 0.00001,
          "throughput": 100,
          "latency": 0.5,
          "updated_at": "2024-09-09"
        },
        "Azure": {
          "model_id": "gpt-4o-2024-05-13",
          "price_per_input_token": 0.0000025,
          "price_per_output_token": 0.00001,
          "throughput": 92,
          "latency": 0.54,
          "updated_at": "2024-09-09"
        }
      }
    },
    {
      "Model": "GPT-4o-2024-08-06",
      "Description": "GPT-4o-2024-08-06 features a 50% price reduction with 78.9% MMLU performance and maintained multimodal capabilities. With improved vision and non-English language support, it offers excellent value for applications requiring multimodal AI at reduced costs.",
      "Meta-description": "GPT-4o-2024-08-06 by OpenAI - 50% price reduction with 78.9% MMLU and enhanced multimodal capabilities. Excellent value for cost-conscious multimodal applications.",
      "OperationalRank": "#20",
      "SafetyRank": "N/A",
      "Org.": "OpenAI",
      "Size": "-",
      "Released": "06-Aug-24",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "$2.50",
      "Output Cost/M": "$10.00",
      "CutoffKnowledge": "2023-10-01",
      "ContextLength": "128,000 tokens",
      "License": "Proprietary",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "~0.6s",
      "GPQA": null,
      "Multimodal": "Yes",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "No",
      "ProviderModelIds": [
        "gpt-4o-2024-08-06"
      ],
      "providers": {
        "OpenAI": {
          "model_id": "gpt-4o-2024-08-06",
          "price_per_input_token": 0.0000025,
          "price_per_output_token": 0.00001,
          "throughput": 132,
          "latency": 0.5,
          "updated_at": "2024-09-09"
        },
        "Azure": {
          "model_id": "gpt-4o-2024-08-06",
          "price_per_input_token": 0.0000025,
          "price_per_output_token": 0.00001,
          "throughput": 99,
          "latency": 0.53,
          "updated_at": "2024-09-09"
        }
      }
    },
    {
      "Model": "o1-2024-12-17",
      "Description": "o1-2024-12-17 represents OpenAI's breakthrough in reasoning AI with 92% MMLU performance and exceptional mathematical capabilities (83% on IMO qualifiers). Operating at 89th percentile on Codeforces competitions, this model excels at complex problem-solving through advanced reasoning processes.",
      "Meta-description": "o1-2024-12-17 by OpenAI - Breakthrough reasoning AI with 92% MMLU and 83% math performance. 89th percentile Codeforces performance through advanced reasoning processes.",
      "OperationalRank": "#2",
      "APIReference": "https://platform.openai.com/docs/api-reference",
      "Playground": "https://platform.openai.com/playground",
      "Documentation": "https://platform.openai.com/docs",
      "SafetyRank": "N/A",
      "Org.": "OpenAI",
      "Size": "-",
      "Released": "17-Dec-24",
      "CodeLMArena": "-",
      "MathLiveBench": "83%",
      "CodeLiveBench": "89%",
      "Input Cost/M": "$27.00",
      "Output Cost/M": "$54.00",
      "CutoffKnowledge": "2023-10-01",
      "ContextLength": "128,000 tokens",
      "License": "Proprietary",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "~10-20s",
      "GPQA": "78.00%",
      "Multimodal": "No",
      "Reasoning": "Yes",
      "Web Access": "No",
      "Fine-tunable": "No",
      "ProviderModelIds": [
        "o1-2024-12-17"
      ],
      "providers": {
        "OpenAI": {
          "model_id": "o1-2024-12-17",
          "price_per_input_token": 0.000015,
          "price_per_output_token": 0.00006,
          "throughput": 66,
          "latency": 16.2,
          "updated_at": "2024-11-24"
        },
        "Azure": {
          "model_id": "o1-2024-12-17",
          "price_per_input_token": 0.000015,
          "price_per_output_token": 0.00006,
          "throughput": 16,
          "latency": 0.54,
          "updated_at": "2024-11-24"
        }
      }
    },
    {
      "Model": "o1-mini",
      "ModelId": "o1-mini-2024-09-12",
      "Description": "o1-mini delivers OpenAI's reasoning capabilities optimized for speed and cost efficiency. With 250 tokens/s throughput and 75% MMLU performance, it provides 80% cost savings over o1-preview while maintaining strong reasoning abilities for mathematical and coding tasks.",
      "Meta-description": "o1-mini by OpenAI - Optimized reasoning AI with 250 tokens/s speed and 75% MMLU. 80% cost savings with maintained reasoning for math and coding tasks.",
      "OperationalRank": "#16",
      "APIReference": "https://platform.openai.com/docs/api-reference",
      "Playground": "https://platform.openai.com/playground",
      "Documentation": "https://platform.openai.com/docs",
      "SafetyRank": "N/A",
      "Org.": "OpenAI",
      "Size": "-",
      "Released": "12-Sep-24",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "$3.00",
      "Output Cost/M": "$12.00",
      "CutoffKnowledge": "2023-10-01",
      "ContextLength": "128,000 tokens",
      "MaxOutputTokens": "65,536",
      "License": "Proprietary",
      "SafeResponses": "N/A",
      "SafeResponsesTotalUsed": null,
      "UnsafeResponses": "N/A",
      "UnsafeResponsesTotalUsed": null,
      "JailbreakingResistance": "N/A",
      "JailbreakingTotalUsed": null,
      "Latency": "~1s",
      "GPQA": "60.00%",
      "Multimodal": "No",
      "Reasoning": "Yes",
      "ProviderModelIds": [
        "o1-mini-2024-09-12"
      ],
      "Modalities": {
        "text": {
          "input": true,
          "output": true
        },
        "image": {
          "input": false,
          "output": false
        },
        "audio": {
          "input": false,
          "output": false
        }
      },
      "Features": {
        "streaming": false,
        "function_calling": false,
        "structured_outputs": false,
        "fine_tuning": false,
        "distillation": false,
        "predicted_outputs": false,
        "multimodal": false,
        "reasoning": true
      },
      "Tools": {
        "web_search": false,
        "file_search": false,
        "image_generation": false,
        "code_interpreter": false,
        "mcp": false,
        "computer_use": false
      },
      "Web Access": "No",
      "Fine-tunable": "No",
      "providers": {
        "OpenAI": {
          "model_id": "o1-mini-2024-09-12",
          "price_per_input_token": 0.000003,
          "price_per_output_token": 0.000012,
          "throughput": 115,
          "latency": 5.2,
          "updated_at": "2024-09-12"
        },
        "Azure": {
          "model_id": "o1-mini-2024-09-12",
          "price_per_input_token": 0.0000033,
          "price_per_output_token": 0.0000132,
          "throughput": 100,
          "latency": 0.5,
          "updated_at": "2024-09-12"
        }
      }
    },
    {
      "Model": "o1-preview",
      "ModelId": "o1-preview-2024-09-12",
      "Description": "o1-preview was OpenAI's limited-release reasoning model featuring 92.3% MMLU performance and exceptional problem-solving capabilities. With 147 tokens/s throughput but high 22.8s latency, it demonstrated advanced reasoning for research applications before full o1 release.",
      "Meta-description": "o1-preview by OpenAI - Limited-release reasoning model with 92.3% MMLU and exceptional problem-solving. Advanced reasoning demonstration for research applications.",
      "OperationalRank": "#4",
      "APIReference": "https://platform.openai.com/docs/api-reference",
      "Playground": "https://platform.openai.com/playground",
      "Documentation": "https://platform.openai.com/docs",
      "SafetyRank": "N/A",
      "Org.": "OpenAI",
      "Size": "-",
      "Released": "12-Sep-24",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "$15.00",
      "Output Cost/M": "$60.00",
      "CutoffKnowledge": "2023-10-01",
      "ContextLength": "128,000 tokens",
      "MaxOutputTokens": "32,768",
      "License": "Proprietary",
      "SafeResponses": "N/A",
      "SafeResponsesTotalUsed": null,
      "UnsafeResponses": "N/A",
      "UnsafeResponsesTotalUsed": null,
      "JailbreakingResistance": "N/A",
      "JailbreakingTotalUsed": null,
      "Latency": "~22.8s",
      "GPQA": "73.30%",
      "Multimodal": "No",
      "Reasoning": "Yes",
      "ProviderModelIds": [
        "o1-preview-2024-09-12"
      ],
      "Modalities": {
        "text": {
          "input": true,
          "output": true
        },
        "image": {
          "input": false,
          "output": false
        },
        "audio": {
          "input": false,
          "output": false
        }
      },
      "Features": {
        "streaming": false,
        "function_calling": false,
        "structured_outputs": false,
        "fine_tuning": false,
        "distillation": false,
        "predicted_outputs": false,
        "multimodal": false,
        "reasoning": true
      },
      "Tools": {
        "web_search": false,
        "file_search": false,
        "image_generation": false,
        "code_interpreter": false,
        "mcp": false,
        "computer_use": false
      },
      "Web Access": "No",
      "Fine-tunable": "No",
      "providers": {
        "OpenAI": {
          "model_id": "o1-preview-2024-09-12",
          "price_per_input_token": 0.000015,
          "price_per_output_token": 0.00006,
          "throughput": 66,
          "latency": 16.2,
          "updated_at": "2024-09-12"
        },
        "Azure": {
          "model_id": "o1-preview-2024-09-12",
          "price_per_input_token": 0.0000165,
          "price_per_output_token": 0.000066,
          "throughput": 16,
          "latency": 0.54,
          "updated_at": "2024-09-12"
        }
      }
    },
    {
      "Model": "o1-pro",
      "ModelId": "o1-pro-2024-12-05",
      "Description": "o1-pro combines OpenAI's advanced reasoning capabilities with ChatGPT tools and browsing integration. Featuring 92% MMLU performance and advanced reasoning with tools, it's designed for professional applications requiring complex problem-solving with real-time information access.",
      "Meta-description": "o1-pro by OpenAI - Advanced reasoning AI with ChatGPT tools integration. 92% MMLU performance for professional applications requiring complex problem-solving.",
      "OperationalRank": "#3",
      "APIReference": "https://platform.openai.com/docs/api-reference",
      "Playground": "https://platform.openai.com/playground",
      "Documentation": "https://platform.openai.com/docs",
      "SafetyRank": "N/A",
      "Org.": "OpenAI",
      "Size": "-",
      "Released": "05-Dec-24",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "$60.00",
      "Output Cost/M": "$240.00",
      "CutoffKnowledge": "2024-10-01",
      "ContextLength": "128,000 tokens",
      "MaxOutputTokens": "32,768",
      "License": "Proprietary",
      "SafeResponses": "N/A",
      "SafeResponsesTotalUsed": null,
      "UnsafeResponses": "N/A",
      "UnsafeResponsesTotalUsed": null,
      "JailbreakingResistance": "N/A",
      "JailbreakingTotalUsed": null,
      "Latency": "~20s",
      "GPQA": "79.00%",
      "Multimodal": "Yes",
      "Reasoning": "Yes",
      "ProviderModelIds": [
        "o1-pro-2024-12-05"
      ],
      "Modalities": {
        "text": {
          "input": true,
          "output": true
        },
        "image": {
          "input": true,
          "output": false
        },
        "audio": {
          "input": false,
          "output": false
        }
      },
      "Features": {
        "streaming": false,
        "function_calling": false,
        "structured_outputs": false,
        "fine_tuning": false,
        "distillation": false,
        "predicted_outputs": false,
        "multimodal": true,
        "reasoning": true
      },
      "Tools": {
        "web_search": true,
        "file_search": false,
        "image_generation": false,
        "code_interpreter": true,
        "mcp": false,
        "computer_use": false
      },
      "Web Access": "Yes",
      "Fine-tunable": "No",
      "providers": {
        "OpenAI": {
          "model_id": "o1-pro-2024-12-05",
          "price_per_input_token": 0.00006,
          "price_per_output_token": 0.00024,
          "throughput": 50,
          "latency": 20,
          "updated_at": "2024-12-05"
        }
      }
    },
    {
      "Model": "o3-2025-04-16",
      "ModelId": "o3-2025-04-16",
      "Description": "o3-2025-04-16 achieves tied #1 ranking on LMArena with 1442 Elo score and 87% MMLU performance. With excellent mathematical and multilingual capabilities, it represents OpenAI's latest breakthrough in conversational AI, rivaling the best models available.",
      "Meta-description": "o3-2025-04-16 by OpenAI - Tied #1 LMArena ranking with 1442 Elo and 87% MMLU. Latest breakthrough in conversational AI with excellent math and multilingual capabilities.",
      "OperationalRank": "#1",
      "APIReference": "https://platform.openai.com/docs/api-reference",
      "Playground": "https://platform.openai.com/playground",
      "Documentation": "https://platform.openai.com/docs",
      "SafetyRank": "N/A",
      "Org.": "OpenAI",
      "Size": "-",
      "Released": "16-Apr-25",
      "CodeLMArena": "1442",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "$5.00",
      "Output Cost/M": "$10.00",
      "CutoffKnowledge": "2024-10-01",
      "ContextLength": "128,000 tokens",
      "MaxOutputTokens": "32,768",
      "License": "Proprietary",
      "SafeResponses": "N/A",
      "SafeResponsesTotalUsed": null,
      "UnsafeResponses": "N/A",
      "UnsafeResponsesTotalUsed": null,
      "JailbreakingResistance": "N/A",
      "JailbreakingTotalUsed": null,
      "Latency": "~0.5-1s",
      "GPQA": "83.30%",
      "Multimodal": "Yes",
      "Reasoning": "Yes",
      "ProviderModelIds": [
        "o3-2025-04-16"
      ],
      "Modalities": {
        "text": {
          "input": true,
          "output": true
        },
        "image": {
          "input": true,
          "output": false
        },
        "audio": {
          "input": false,
          "output": false
        }
      },
      "Features": {
        "streaming": true,
        "function_calling": true,
        "structured_outputs": true,
        "fine_tuning": false,
        "distillation": false,
        "predicted_outputs": false,
        "multimodal": true,
        "reasoning": true
      },
      "Tools": {
        "web_search": false,
        "file_search": false,
        "image_generation": false,
        "code_interpreter": false,
        "mcp": false,
        "computer_use": false
      },
      "Web Access": "No",
      "Fine-tunable": "No",
      "providers": {
        "OpenAI": {
          "model_id": "o3-2025-04-16",
          "price_per_input_token": 0.00001,
          "price_per_output_token": 0.00004,
          "throughput": 50,
          "latency": 20,
          "updated_at": "2025-04-16"
        }
      }
    },
    {
      "Model": "o3-mini",
      "ModelId": "o3-mini-2024-11-24",
      "Description": "o3-mini provides OpenAI's next-generation capabilities in a compact, efficient package. With 1255 Elo score and 80.7% MMLU performance, it offers strong reasoning and coding abilities optimized for speed and cost-effectiveness in production applications.",
      "Meta-description": "o3-mini by OpenAI - Next-generation compact AI with 1255 Elo and 80.7% MMLU. Strong reasoning and coding optimized for speed and cost-effectiveness.",
      "OperationalRank": "#18",
      "APIReference": "https://platform.openai.com/docs/api-reference",
      "Playground": "https://platform.openai.com/playground",
      "Documentation": "https://platform.openai.com/docs",
      "SafetyRank": "N/A",
      "Org.": "OpenAI",
      "Size": "-",
      "Released": "16-Apr-25",
      "CodeLMArena": "1255",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "-",
      "Output Cost/M": "-",
      "CutoffKnowledge": "2024-10-01",
      "ContextLength": "128,000 tokens",
      "MaxOutputTokens": "65,536",
      "License": "Proprietary",
      "SafeResponses": "N/A",
      "SafeResponsesTotalUsed": null,
      "UnsafeResponses": "N/A",
      "UnsafeResponsesTotalUsed": null,
      "JailbreakingResistance": "N/A",
      "JailbreakingTotalUsed": null,
      "Latency": "~0.3s",
      "GPQA": "77.20%",
      "Multimodal": "No",
      "Reasoning": "Yes",
      "ProviderModelIds": [
        "o3-mini-2024-11-24"
      ],
      "Modalities": {
        "text": {
          "input": true,
          "output": true
        },
        "image": {
          "input": false,
          "output": false
        },
        "audio": {
          "input": false,
          "output": false
        }
      },
      "Features": {
        "streaming": false,
        "function_calling": false,
        "structured_outputs": false,
        "fine_tuning": false,
        "distillation": false,
        "predicted_outputs": false,
        "multimodal": false,
        "reasoning": true
      },
      "Tools": {
        "web_search": false,
        "file_search": false,
        "image_generation": false,
        "code_interpreter": false,
        "mcp": false,
        "computer_use": false
      },
      "Web Access": "No",
      "Fine-tunable": "No",
      "providers": {
        "OpenAI": {
          "model_id": "o3-mini-2024-11-24",
          "price_per_input_token": 0.0000011,
          "price_per_output_token": 0.0000044,
          "throughput": 115,
          "latency": 5.2,
          "updated_at": "2024-11-24"
        },
        "Azure": {
          "model_id": "o3-mini-2024-11-24",
          "price_per_input_token": 0.0000011,
          "price_per_output_token": 0.0000044,
          "throughput": 115,
          "latency": 5.2,
          "updated_at": "2024-11-24"
        }
      }
    },
    {
      "Model": "o4-mini",
      "ModelId": "o4-mini-2024-12-18",
      "Description": "o4-mini represents OpenAI's latest advancement in efficient AI models with 1208 Elo ranking and enhanced vision task capabilities. Designed for next-generation applications, it offers improved reasoning efficiency while maintaining compact size for high-volume deployments.",
      "Meta-description": "o4-mini by OpenAI - Latest efficient AI with 1208 Elo and enhanced vision capabilities. Next-generation model with improved reasoning efficiency for high-volume deployments.",
      "OperationalRank": "#24",
      "APIReference": "https://platform.openai.com/docs/api-reference",
      "Playground": "https://platform.openai.com/playground",
      "Documentation": "https://platform.openai.com/docs",
      "SafetyRank": "N/A",
      "Org.": "OpenAI",
      "Size": "-",
      "Released": "01-May-25",
      "CodeLMArena": "1208",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "-",
      "Output Cost/M": "-",
      "CutoffKnowledge": "2024-12-01",
      "ContextLength": "128,000 tokens",
      "MaxOutputTokens": "65,536",
      "License": "Proprietary",
      "SafeResponses": "N/A",
      "SafeResponsesTotalUsed": null,
      "UnsafeResponses": "N/A",
      "UnsafeResponsesTotalUsed": null,
      "JailbreakingResistance": "N/A",
      "JailbreakingTotalUsed": null,
      "Latency": "~0.3s",
      "GPQA": "81.40%",
      "Multimodal": "Yes",
      "Reasoning": "Yes",
      "ProviderModelIds": [
        "o4-mini-2024-12-18"
      ],
      "Modalities": {
        "text": {
          "input": true,
          "output": true
        },
        "image": {
          "input": true,
          "output": false
        },
        "audio": {
          "input": false,
          "output": false
        }
      },
      "Features": {
        "streaming": false,
        "function_calling": false,
        "structured_outputs": false,
        "fine_tuning": false,
        "distillation": false,
        "predicted_outputs": false,
        "multimodal": true,
        "reasoning": true
      },
      "Tools": {
        "web_search": false,
        "file_search": false,
        "image_generation": false,
        "code_interpreter": false,
        "mcp": false,
        "computer_use": false
      },
      "Web Access": "No",
      "Fine-tunable": "No",
      "providers": {
        "OpenAI": {
          "model_id": "o4-mini-2024-12-18",
          "price_per_input_token": 0.0000011,
          "price_per_output_token": 0.0000044,
          "throughput": 115,
          "latency": 5.2,
          "updated_at": "2024-12-18"
        }
      }
    },
    {
      "Model": "Claude 3 Haiku",
      "Description": "Claude 3 Haiku is Anthropic's fastest and most cost-effective language model, designed for rapid response times and efficient processing. With 20B parameters and a 200K token context window, it excels at quick tasks like content summarization, customer support, and real-time applications while maintaining Anthropic's commitment to AI safety.",
      "Meta-description": "Claude 3 Haiku by Anthropic - Fast, cost-effective 20B parameter AI model with 200K context. Perfect for customer support, content summarization, and rapid AI applications. Ranked #21 operationally.",
      "OperationalRank": "#21",
      "SafetyRank": "N/A",
      "Org.": "Anthropic",
      "Size": "20B Parameters",
      "Released": "07-Mar-24",
      "CodeLMArena": "1184",
      "MathLiveBench": "22.90%",
      "CodeLiveBench": "-",
      "Input Cost/M": "$0.25 ",
      "Output Cost/M": "$1.25 ",
      "CutoffKnowledge": "2023-08-01",
      "ContextLength": "200,000 tokens",
      "License": "Proprietary",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": "33.30%",
      "Multimodal": "Yes",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "No",
      "ProviderModelIds": [
        "claude-3-haiku-20240307"
      ]
    },
    {
      "Model": "Claude 3 Opus",
      "Description": "Claude 3 Opus represents Anthropic's most powerful and sophisticated language model with 2T parameters. Built for complex reasoning, creative tasks, and advanced problem-solving, it delivers exceptional performance across diverse domains while maintaining strong safety measures and ethical AI principles.",
      "Meta-description": "Claude 3 Opus by Anthropic - Most powerful 2T parameter AI model with advanced reasoning capabilities. Ideal for complex analysis, creative writing, and sophisticated problem-solving tasks.",
      "OperationalRank": "#76",
      "SafetyRank": "N/A",
      "Org.": "Anthropic",
      "Size": "2T Parameters",
      "Released": "04-Mar-24",
      "CodeLMArena": "1236",
      "MathLiveBench": "43.40%",
      "CodeLiveBench": "-",
      "Input Cost/M": "$15 ",
      "Output Cost/M": "$75 ",
      "CutoffKnowledge": "2023-08-01",
      "ContextLength": "200,000 tokens",
      "License": "Proprietary",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": "50.40%",
      "Multimodal": "Yes",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "No",
      "ProviderModelIds": [
        "claude-3-opus-20240229"
      ]
    },
    {
      "Model": "Claude 3.5 Haiku",
      "Description": "Claude 3.5 Haiku combines speed and intelligence in Anthropic's updated compact model. With improved 20B parameters and enhanced coding capabilities (28% CodeLiveBench), it offers better performance than its predecessor while maintaining rapid response times and cost efficiency for production applications.",
      "Meta-description": "Claude 3.5 Haiku by Anthropic - Enhanced 20B parameter model with improved coding abilities (28% CodeLiveBench). Fast, efficient AI for production applications and development tasks.",
      "OperationalRank": "#26",
      "SafetyRank": "N/A",
      "Org.": "Anthropic",
      "Size": "20B Parameters",
      "Released": "22-Oct-24",
      "CodeLMArena": "1263",
      "MathLiveBench": "35.50%",
      "CodeLiveBench": "28.00%",
      "Input Cost/M": "$1 ",
      "Output Cost/M": "$5 ",
      "CutoffKnowledge": "2024-07-01",
      "ContextLength": "200,000 tokens",
      "License": "Proprietary",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": "41.60%",
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "No",
      "ProviderModelIds": [
        "claude-3-5-haiku-20241022"
      ]
    },
    {
      "Model": "Claude 3.5 Sonnet",
      "Description": "Claude 3.5 Sonnet delivers exceptional performance with 70B parameters, achieving strong results in coding (32.3% CodeLiveBench) and mathematical reasoning (51.3% MathLiveBench). This balanced model excels at complex analysis, software development, and professional writing while maintaining Anthropic's safety standards.",
      "Meta-description": "Claude 3.5 Sonnet by Anthropic - 70B parameter model with excellent coding (32.3%) and math abilities (51.3%). Perfect for software development, analysis, and professional applications.",
      "OperationalRank": "#34",
      "SafetyRank": "N/A",
      "Org.": "Anthropic",
      "Size": "70B Parameters",
      "Released": "22-Oct-24",
      "CodeLMArena": "1313",
      "MathLiveBench": "51.30%",
      "CodeLiveBench": "32.30%",
      "Input Cost/M": "$3 ",
      "Output Cost/M": "$15 ",
      "CutoffKnowledge": "2024-04-01",
      "ContextLength": "200,000 tokens",
      "License": "Proprietary",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": "67.20%",
      "Multimodal": "Yes",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "No",
      "ProviderModelIds": [
        "claude-3-5-sonnet-20241022",
        "claude-3-5-sonnet-20240620"
      ]
    },
    {
      "Model": "Claude 3.7 Sonnet",
      "Description": "Claude 3.7 Sonnet sets the gold standard for AI safety with #1 safety ranking, achieving 100% safe responses and perfect jailbreaking resistance. With enhanced 70B parameters and superior mathematical reasoning (63.3% MathLiveBench), it's the ideal choice for enterprise applications requiring maximum safety and reliability.",
      "Meta-description": "Claude 3.7 Sonnet by Anthropic - #1 safest AI model with 100% safe responses and perfect security. 70B parameters with excellent math skills (63.3%). Enterprise-grade safety and performance.",
      "OperationalRank": "#31",
      "SafetyRank": "#1",
      "Org.": "Anthropic",
      "Size": "70B Parameters",
      "Released": "24-Feb-25",
      "CodeLMArena": "1326",
      "MathLiveBench": "63.30%",
      "CodeLiveBench": "32.40%",
      "Input Cost/M": "$3 ",
      "Output Cost/M": "$15 ",
      "CutoffKnowledge": "2024-10-01",
      "ContextLength": "200,000 tokens",
      "License": "Proprietary",
      "SafeResponses": "100%",
      "SafeResponsesTotalUsed": 300,
      "UnsafeResponses": "0%",
      "UnsafeResponsesTotalUsed": 300,
      "JailbreakingResistance": "100%",
      "JailbreakingTotalUsed": 37,
      "Latency": "-",
      "GPQA": "84.80%",
      "Multimodal": "Yes",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "No",
      "ProviderModelIds": [
        "claude-3-7-sonnet-20250219"
      ]
    },
    {
      "Model": "Claude 3.7 Sonnet Thinking",
      "Description": "Claude 3.7 Sonnet Thinking incorporates advanced reasoning capabilities with explicit thinking processes. Featuring 70B parameters and exceptional mathematical performance (77.5% MathLiveBench), this model excels at complex problem-solving, step-by-step analysis, and transparent reasoning for research and educational applications.",
      "Meta-description": "Claude 3.7 Sonnet Thinking by Anthropic - Advanced 70B parameter model with transparent reasoning. Exceptional math performance (77.5%) and step-by-step problem-solving capabilities.",
      "OperationalRank": "#30",
      "SafetyRank": "N/A",
      "Org.": "Anthropic",
      "Size": "70B Parameters",
      "Released": "24-Feb-25",
      "CodeLMArena": "1333",
      "MathLiveBench": "77.50%",
      "CodeLiveBench": "44.70%",
      "Input Cost/M": "$3 ",
      "Output Cost/M": "$15 ",
      "CutoffKnowledge": "2024-10-01",
      "ContextLength": "200,000 tokens",
      "License": "Proprietary",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": "84.80%",
      "Multimodal": "No",
      "Reasoning": "Yes",
      "Web Access": "No",
      "Fine-tunable": "No"
    },
    {
      "Model": "Codestral 25.01",
      "Description": "Codestral 25.01 is Mistral's latest specialized coding model, designed specifically for software development tasks. With an extended 256K token context window, it handles large codebases and complex programming projects efficiently, making it ideal for developers and software engineering teams.",
      "Meta-description": "Codestral 25.01 by Mistral - Specialized coding AI model with 256K context window. Perfect for software development, large codebase analysis, and programming assistance.",
      "OperationalRank": "#50",
      "SafetyRank": "N/A",
      "Org.": "Mistral",
      "Size": "-",
      "Released": "14-Jan-25",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "-",
      "Output Cost/M": "-",
      "CutoffKnowledge": null,
      "ContextLength": "256,000 tokens",
      "License": "Proprietary",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "No"
    },
    {
      "Model": "DeepSeek 2.5",
      "Description": "DeepSeek 2.5 delivers impressive performance with 236B parameters at an exceptional value proposition. Ranked #9 operationally, it offers strong coding capabilities and cost-effective pricing, making it an attractive option for businesses seeking high performance without premium costs.",
      "Meta-description": "DeepSeek 2.5 - High-performance 236B parameter AI model ranked #9 globally. Cost-effective solution for coding, analysis, and business applications with excellent value.",
      "OperationalRank": "#9",
      "SafetyRank": "N/A",
      "Org.": "DeepSeek",
      "Size": "236B Parameters",
      "Released": "12-Sep-24",
      "CodeLMArena": "1255",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "$0.13 ",
      "Output Cost/M": "$0.38 ",
      "CutoffKnowledge": "2023-11-01",
      "ContextLength": "128,000 tokens",
      "License": "Proprietary",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "No",
      "ProviderModelIds": [
        "deepseek-v2.5"
      ]
    },
    {
      "Model": "DeepSeek Coder 2",
      "Description": "DeepSeek Coder 2 is a specialized programming model with 236B parameters, optimized for software development tasks. It provides excellent coding assistance at competitive pricing, making it ideal for developers, coding bootcamps, and software companies seeking reliable programming AI support.",
      "Meta-description": "DeepSeek Coder 2 - Specialized 236B parameter coding AI model. Excellent programming assistance at competitive prices. Perfect for developers and software development teams.",
      "OperationalRank": "#10",
      "SafetyRank": "N/A",
      "Org.": "DeepSeek",
      "Size": "236B Parameters",
      "Released": "22-Jul-24",
      "CodeLMArena": "1226",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "$0.14 ",
      "Output Cost/M": "$0.28 ",
      "CutoffKnowledge": "2023-11-01",
      "ContextLength": "128,000 tokens",
      "License": "Proprietary",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "No"
    },
    {
      "Model": "DeepSeek R1",
      "Description": "DeepSeek R1 represents a breakthrough in reasoning AI with 671B parameters and exceptional mathematical capabilities (79.5% MathLiveBench). Ranked #5 in safety with strong protective measures (89% safe responses), it combines advanced reasoning with responsible AI deployment for complex analytical tasks.",
      "Meta-description": "DeepSeek R1 - Advanced 671B parameter reasoning AI with exceptional math skills (79.5%) and #5 safety ranking. Perfect for complex analysis and mathematical problem-solving.",
      "OperationalRank": "#14",
      "SafetyRank": "#7",
      "Org.": "DeepSeek",
      "Size": "671B Parameters",
      "Released": "20-Jan-25",
      "CodeLMArena": "-",
      "MathLiveBench": "79.50%",
      "CodeLiveBench": "48.50%",
      "Input Cost/M": "$0.55 ",
      "Output Cost/M": "$2.19 ",
      "CutoffKnowledge": "2024-07-01",
      "ContextLength": "128,000 tokens",
      "License": "Open Source",
      "SafeResponses": "89%",
      "SafeResponsesTotalUsed": 300,
      "UnsafeResponses": "11%",
      "UnsafeResponsesTotalUsed": 300,
      "JailbreakingResistance": "32%",
      "JailbreakingTotalUsed": 37,
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "No",
      "Reasoning": "Yes",
      "Web Access": "No",
      "Fine-tunable": "Yes",
      "ProviderModelIds": [
        "deepseek-r1"
      ]
    },
    {
      "Model": "DeepSeek V3",
      "Description": "DeepSeek V3 is a massive 685B parameter open-source model that delivers exceptional performance in mathematical reasoning (73.5% MathLiveBench) and coding tasks (40.5% CodeLiveBench). As an open-source solution, it provides enterprise-level capabilities with transparency and customization options.",
      "Meta-description": "DeepSeek V3 - Massive 685B parameter open-source AI model with excellent math (73.5%) and coding (40.5%) performance. Enterprise capabilities with open-source transparency.",
      "OperationalRank": "#13",
      "SafetyRank": "N/A",
      "Org.": "DeepSeek",
      "Size": "685B Parameters",
      "Released": "24-Mar-25",
      "CodeLMArena": "-",
      "MathLiveBench": "73.50%",
      "CodeLiveBench": "40.50%",
      "Input Cost/M": "$0.27 ",
      "Output Cost/M": "$1.10 ",
      "CutoffKnowledge": "2024-07-01",
      "ContextLength": "128,000 tokens",
      "License": "Open Source",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "Yes",
      "ProviderModelIds": [
        "deepseek-v3"
      ]
    },
    {
      "Model": "DeepSeek V3",
      "Description": "DeepSeek V3 (671B variant) offers remarkable value with 671B parameters and strong mathematical reasoning (60.5% MathLiveBench). With extremely competitive pricing and open-source accessibility, it democratizes access to high-performance AI for researchers, developers, and organizations worldwide.",
      "Meta-description": "DeepSeek V3 671B - Cost-effective open-source AI model with strong math performance (60.5%). Democratizing high-performance AI with competitive pricing and transparency.",
      "OperationalRank": "#13",
      "SafetyRank": "N/A",
      "Org.": "DeepSeek",
      "Size": "671B Parameters",
      "Released": "26-Dec-24",
      "CodeLMArena": "1279",
      "MathLiveBench": "60.50%",
      "CodeLiveBench": "-",
      "Input Cost/M": "$0.07 ",
      "Output Cost/M": "$1.10 ",
      "CutoffKnowledge": "2024-07-01",
      "ContextLength": "128,000 tokens",
      "License": "Open Source",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "Yes"
    },
    {
      "Model": "Gemini 2.5 Pro Preview 06-05",
      "Description": "Gemini 2.5 Pro Preview 06-05 represents Google's most advanced model preview with exceptional GPQA performance (86.4%) and multimodal capabilities. This preview version showcases cutting-edge AI research and provides early access to next-generation language understanding and visual processing.",
      "Meta-description": "gemini-2.5-pro-preview-0605 by Google - Most advanced model preview with exceptional GPQA (86.4%) and multimodal capabilities. Early access to next-generation AI research.",
      "OperationalRank": "-",
      "SafetyRank": "N/A",
      "Org.": "Google",
      "Size": "-",
      "Released": "05-Jun-25",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "$1.25",
      "Output Cost/M": "$10.00",
      "CutoffKnowledge": "2025-01-31",
      "ContextLength": "1,048,576 tokens",
      "License": "Proprietary",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": "86.40%",
      "Multimodal": "Yes",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "No"
    },
    {
      "Model": "Gemini 2.5 Flash Preview 05-20",
      "Description": "Gemini 2.5 Flash represents Google's latest speed innovation with exceptional GPQA performance (82.8%) and multimodal capabilities. This cutting-edge model combines rapid response times with advanced capabilities, making it ideal for modern development and real-time applications.",
      "Meta-description": "gemini-2.5-flash-exp-0827 by Google - Latest speed model with exceptional GPQA (82.8%) and multimodal capabilities. Rapid response for modern development applications.",
      "OperationalRank": "#4",
      "SafetyRank": "#14",
      "Org.": "Google",
      "Size": "-",
      "Released": "17-Apr-25",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "$0.15",
      "Output Cost/M": "$0.60",
      "CutoffKnowledge": "2025-01-31",
      "ContextLength": "1,048,576 tokens",
      "License": "Proprietary",
      "SafeResponses": "85%",
      "SafeResponsesTotalUsed": 99,
      "UnsafeResponses": "14%",
      "UnsafeResponsesTotalUsed": 99,
      "JailbreakingResistance": "12%",
      "JailbreakingTotalUsed": 100,
      "Latency": "-",
      "GPQA": "82.80%",
      "Multimodal": "Yes",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "No"
    },
    {
      "Model": "Gemini 2.0 Flash Thinking",
      "Description": "Gemini 2.0 Flash Thinking is Google's reasoning-focused model with strong GPQA performance (74.2%) and multimodal capabilities. This specialized model excels at complex reasoning tasks while maintaining rapid response times for applications requiring deep analytical thinking.",
      "Meta-description": "Gemini 2.0 Flash Thinking by Google - Reasoning-focused model with strong GPQA (74.2%) and multimodal capabilities. Excels at complex reasoning and analytical thinking.",
      "OperationalRank": "-",
      "SafetyRank": "N/A",
      "Org.": "Google",
      "Size": "-",
      "Released": "01-Aug-24",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "-",
      "Output Cost/M": "-",
      "CutoffKnowledge": "2024-08-01",
      "ContextLength": "1,000,000 tokens",
      "License": "Proprietary",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": "74.20%",
      "Multimodal": "Yes",
      "Reasoning": "Yes",
      "Web Access": "No",
      "Fine-tunable": "No"
    },
    {
      "Model": "gemini-2.0-flash-exp-1206",
      "Description": "Gemini 2.0 Flash represents Google's next-generation speed model with enhanced capabilities and strong GPQA performance (62.1%). It delivers excellent performance while maintaining rapid response times and competitive pricing for diverse applications.",
      "Meta-description": "gemini-2.0-flash-exp-1206 by Google - Next-gen speed model with strong GPQA (62.1%) and enhanced capabilities. Excellent performance with rapid response times.",
      "OperationalRank": "#3",
      "SafetyRank": "#12",
      "Org.": "Google",
      "Size": "-",
      "Released": "05-Feb-25",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "$0.10",
      "Output Cost/M": "$0.40",
      "CutoffKnowledge": "2024-08-01",
      "ContextLength": "1,048,576 tokens",
      "License": "Proprietary",
      "SafeResponses": "89%",
      "SafeResponsesTotalUsed": 99,
      "UnsafeResponses": "10%",
      "UnsafeResponsesTotalUsed": 99,
      "JailbreakingResistance": "10%",
      "JailbreakingTotalUsed": 100,
      "Latency": "-",
      "GPQA": "62.10%",
      "Multimodal": "Yes",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "No"
    },
    {
      "Model": "Gemini 1.5 Pro",
      "Description": "Gemini 1.5 Pro is Google's flagship professional model with strong GPQA performance (59.1%) and excellent MMLU scores (85.9%). With multimodal capabilities and extensive context window, it's designed for complex enterprise applications requiring advanced reasoning.",
      "Meta-description": "Gemini 1.5 Pro by Google - Flagship professional AI with strong GPQA (59.1%) and excellent MMLU (85.9%) performance. Advanced reasoning for enterprise applications.",
      "OperationalRank": "#18",
      "SafetyRank": "N/A",
      "Org.": "Google",
      "Size": "-",
      "Released": "01-Nov-23",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "$2.50",
      "Output Cost/M": "$10.00",
      "CutoffKnowledge": "2023-11-01",
      "ContextLength": "2,097,152 tokens",
      "License": "Proprietary",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": "59.10%",
      "MMLU": "85.90%",
      "Multimodal": "Yes",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "No"
    },
    {
      "Model": "Gemini 2.0 Flash-Lite",
      "Description": "Gemini 2.0 Flash-Lite offers a streamlined version of Google's advanced model with solid GPQA performance (51.5%) and multimodal capabilities. With cost-effective pricing, it's ideal for applications requiring good performance without premium costs.",
      "Meta-description": "Gemini 2.0 Flash-Lite by Google - Streamlined AI model with solid GPQA (51.5%) and multimodal capabilities. Cost-effective performance for budget-conscious applications.",
      "OperationalRank": "#5",
      "SafetyRank": "N/A",
      "Org.": "Google",
      "Size": "-",
      "Released": "01-Jun-24",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "$0.07",
      "Output Cost/M": "$0.30",
      "CutoffKnowledge": "2024-06-01",
      "ContextLength": "1,048,576 tokens",
      "License": "Proprietary",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": "51.50%",
      "Multimodal": "Yes",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "No"
    },
    {
      "Model": "Gemini 1.5 Flash",
      "Description": "Gemini 1.5 Flash is Google's speed-optimized model with solid GPQA performance (51.0%) and strong MMLU scores (78.9%). With multimodal capabilities and extensive context, it's perfect for real-time applications and high-volume processing tasks.",
      "Meta-description": "Gemini 1.5 Flash by Google - Speed-optimized AI with solid GPQA (51.0%) and strong MMLU (78.9%) performance. Perfect for real-time and high-volume applications.",
      "OperationalRank": "#2",
      "SafetyRank": "N/A",
      "Org.": "Google",
      "Size": "-",
      "Released": "01-Nov-23",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "$0.15",
      "Output Cost/M": "$0.60",
      "CutoffKnowledge": "2023-11-01",
      "ContextLength": "1,048,576 tokens",
      "License": "Proprietary",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": "51.00%",
      "MMLU": "78.90%",
      "Multimodal": "Yes",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "No"
    },
    {
      "Model": "Gemma 3 27B",
      "Description": "Gemma 3 27B delivers high-performance capabilities with 27B parameters and strong GPQA performance (42.4%). This latest-generation open-source model provides enterprise-level performance with multimodal capabilities for complex applications.",
      "Meta-description": "Gemma 3 27B by Google - High-performance 27B parameter open-source model with strong GPQA (42.4%) and multimodal capabilities. Enterprise-level performance.",
      "OperationalRank": "#29",
      "SafetyRank": "N/A",
      "Org.": "Google",
      "Size": "27B Parameters",
      "Released": "01-Aug-24",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "$0.10",
      "Output Cost/M": "$0.20",
      "CutoffKnowledge": "2024-08",
      "ContextLength": "131,072 tokens",
      "License": "Open Source",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": "42.40%",
      "Multimodal": "Yes",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "Yes"
    },
    {
      "Model": "Gemma 3 12B",
      "Description": "Gemma 3 12B represents Google's balanced open-source model with 12B parameters and solid GPQA performance (40.9%). With multimodal capabilities and cost-effective pricing, it provides excellent value for diverse applications.",
      "Meta-description": "Gemma 3 12B by Google - Balanced 12B parameter open-source model with solid GPQA (40.9%) and multimodal capabilities. Excellent value for diverse applications.",
      "OperationalRank": "#56",
      "SafetyRank": "N/A",
      "Org.": "Google",
      "Size": "12B Parameters",
      "Released": "01-Aug-24",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "$0.05",
      "Output Cost/M": "$0.10",
      "CutoffKnowledge": "2024-08",
      "ContextLength": "131,072 tokens",
      "License": "Open Source",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": "40.90%",
      "Multimodal": "Yes",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "Yes"
    },
    {
      "Model": "Gemini Diffusion",
      "Description": "Gemini Diffusion is Google's specialized model for image generation and visual tasks with GPQA performance (40.4%). This model focuses on creative applications and visual content generation without multimodal input capabilities.",
      "Meta-description": "Gemini Diffusion by Google - Specialized model for image generation with GPQA (40.4%) performance. Focused on creative applications and visual content generation.",
      "OperationalRank": "-",
      "SafetyRank": "N/A",
      "Org.": "Google",
      "Size": "-",
      "Released": "01-Jan-24",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "-",
      "Output Cost/M": "-",
      "CutoffKnowledge": "-",
      "ContextLength": "32,768 tokens",
      "License": "Proprietary",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": "40.40%",
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "No"
    },
    {
      "Model": "Gemini 1.5 Flash 8B",
      "Description": "Gemini 1.5 Flash 8B is Google's compact speed model with 8B parameters and solid GPQA performance (38.4%). With multimodal capabilities and ultra-low pricing, it's perfect for high-volume applications requiring cost efficiency.",
      "Meta-description": "Gemini 1.5 Flash 8B by Google - Compact 8B parameter speed model with solid GPQA (38.4%) and multimodal capabilities. Perfect for cost-efficient high-volume applications.",
      "OperationalRank": "-",
      "SafetyRank": "N/A",
      "Org.": "Google",
      "Size": "8B Parameters",
      "Released": "01-Oct-24",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "$0.07",
      "Output Cost/M": "$0.30",
      "CutoffKnowledge": "2024-10-01",
      "ContextLength": "1,048,576 tokens",
      "License": "Proprietary",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": "38.40%",
      "Multimodal": "Yes",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "No"
    },
    {
      "Model": "Gemma 3 4B",
      "Description": "Gemma 3 4B is Google's efficient open-source model with 4B parameters and solid GPQA performance (30.8%). With multimodal capabilities and ultra-low pricing, it enables AI functionality in resource-constrained environments.",
      "Meta-description": "Gemma 3 4B by Google - Efficient 4B parameter open-source model with solid GPQA (30.8%) and multimodal capabilities. AI for resource-constrained environments.",
      "OperationalRank": "#55",
      "SafetyRank": "N/A",
      "Org.": "Google",
      "Size": "4B Parameters",
      "Released": "01-Aug-24",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "$0.02",
      "Output Cost/M": "$0.04",
      "CutoffKnowledge": "2024-08-01",
      "ContextLength": "131,072 tokens",
      "License": "Open Source",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": "30.80%",
      "Multimodal": "Yes",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "Yes"
    },
    {
      "Model": "Gemini 1.0 Pro",
      "Description": "Gemini 1.0 Pro is Google's foundational professional model with solid GPQA performance (27.9%) and strong MMLU scores (71.8%). As a text-only model, it provides reliable AI capabilities for applications focused on language understanding.",
      "Meta-description": "Gemini 1.0 Pro by Google - Foundational professional model with solid GPQA (27.9%) and strong MMLU (71.8%) performance. Reliable text-only AI capabilities.",
      "OperationalRank": "-",
      "SafetyRank": "N/A",
      "Org.": "Google",
      "Size": "-",
      "Released": "01-Feb-24",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "$0.50",
      "Output Cost/M": "$1.50",
      "CutoffKnowledge": "2024-02-01",
      "ContextLength": "32,760 tokens",
      "License": "Proprietary",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": "27.90%",
      "MMLU": "71.80%",
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "No"
    },
    {
      "Model": "Gemma 3n E2B Instructed LiteRT Preview",
      "Description": "Gemma 3n E2B Instructed LiteRT Preview is Google's experimental lightweight model with 2B parameters, designed for edge computing. With GPQA performance (24.8%) and MMLU scores (60.1%), it enables on-device AI applications with multimodal capabilities.",
      "Meta-description": "Gemma 3n E2B Instructed LiteRT Preview by Google - Experimental 2B parameter edge model with GPQA (24.8%) and MMLU (60.1%). On-device AI with multimodal capabilities.",
      "OperationalRank": "-",
      "SafetyRank": "N/A",
      "Org.": "Google",
      "Size": "2B Parameters",
      "Released": "01-Jun-24",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "-",
      "Output Cost/M": "-",
      "CutoffKnowledge": "2024-06-01",
      "ContextLength": "32,768 tokens",
      "License": "Open Source",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": "24.80%",
      "MMLU": "60.10%",
      "Multimodal": "Yes",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "Yes"
    },
    {
      "Model": "Gemma 3n E4B Instructed LiteRT Preview",
      "Description": "Gemma 3n E4B Instructed LiteRT Preview is Google's experimental lightweight model with 4B parameters for edge computing. With GPQA performance (23.7%) and MMLU scores (64.9%), it provides enhanced on-device AI capabilities with multimodal support.",
      "Meta-description": "Gemma 3n E4B Instructed LiteRT Preview by Google - Experimental 4B parameter edge model with GPQA (23.7%) and MMLU (64.9%). Enhanced on-device AI with multimodal support.",
      "OperationalRank": "-",
      "SafetyRank": "N/A",
      "Org.": "Google",
      "Size": "4B Parameters",
      "Released": "01-Jun-24",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "-",
      "Output Cost/M": "-",
      "CutoffKnowledge": "2024-06-01",
      "ContextLength": "32,768 tokens",
      "License": "Open Source",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": "23.70%",
      "MMLU": "64.90%",
      "Multimodal": "Yes",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "Yes"
    },
    {
      "Model": "Gemma 3 1B",
      "Description": "Gemma 3 1B is Google's most compact third-generation model with 1B parameters and basic GPQA performance (19.2%). Designed for ultra-lightweight applications, it enables AI functionality in extremely resource-constrained environments.",
      "Meta-description": "Gemma 3 1B by Google - Ultra-compact 1B parameter model with basic GPQA (19.2%) performance. AI functionality for extremely resource-constrained environments.",
      "OperationalRank": "#54",
      "SafetyRank": "N/A",
      "Org.": "Google",
      "Size": "1B Parameters",
      "Released": "01-Aug-24",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "-",
      "Output Cost/M": "-",
      "CutoffKnowledge": "-",
      "ContextLength": "32,000 tokens",
      "License": "Open Source",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": "19.20%",
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "Yes"
    },
    {
      "Model": "Gemma 2 27B",
      "Description": "Gemma 2 27B is Google's powerful second-generation open-source model with 27B parameters and strong MMLU performance (75.2%). As a text-only model, it provides excellent language understanding capabilities for research and enterprise applications.",
      "Meta-description": "Gemma 2 27B by Google - Powerful 27B parameter open-source model with strong MMLU (75.2%) performance. Excellent text-only language understanding capabilities.",
      "OperationalRank": "#35",
      "SafetyRank": "N/A",
      "Org.": "Google",
      "Size": "27B Parameters",
      "Released": "27-Jun-24",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "-",
      "Output Cost/M": "-",
      "CutoffKnowledge": "2024-04-01",
      "ContextLength": "8,192 tokens",
      "License": "Open Source",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "MMLU": "75.20%",
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "Yes"
    },
    {
      "Model": "Gemma 2 9B",
      "Description": "Gemma 2 9B is Google's efficient second-generation open-source model with 9B parameters and solid MMLU performance (71.3%). As a text-only model, it provides reliable language understanding capabilities with excellent efficiency.",
      "Meta-description": "Gemma 2 9B by Google - Efficient 9B parameter open-source model with solid MMLU (71.3%) performance. Reliable text-only language understanding with excellent efficiency.",
      "OperationalRank": "#39",
      "SafetyRank": "#9",
      "Org.": "Google",
      "Size": "9B Parameters",
      "Released": "27-Jun-24",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "-",
      "Output Cost/M": "-",
      "CutoffKnowledge": "2024-04-01",
      "ContextLength": "8,192 tokens",
      "License": "Open Source",
      "SafeResponses": "98%",
      "SafeResponsesTotalUsed": 99,
      "UnsafeResponses": "1%",
      "UnsafeResponsesTotalUsed": 99,
      "JailbreakingResistance": "2%",
      "JailbreakingTotalUsed": 37,
      "Latency": "-",
      "MMLU": "71.30%",
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "Yes"
    },
    {
      "Model": "Grok-3",
      "Description": "Grok-3 is xAI's large-scale language model with 300B parameters, designed for unrestricted conversations and creative applications. Ranked #11 in safety with unique characteristics, it offers a different approach to AI interaction with minimal content filtering for specialized use cases.",
      "Meta-description": "Grok-3 by xAI - Large 300B parameter AI model with unrestricted conversation capabilities. Unique approach to AI interaction with minimal filtering for specialized applications.",
      "OperationalRank": "#70",
      "SafetyRank": "#15",
      "Org.": "xAI",
      "Size": "300B Parameters",
      "Released": "01-May-24",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "$3.00",
      "Output Cost/M": "$15.00",
      "CutoffKnowledge": "2024-11-17",
      "ContextLength": "128,000 tokens",
      "License": "Proprietary",
      "SafeResponses": "2.70%",
      "SafeResponsesTotalUsed": 300,
      "UnsafeResponses": "97.30%",
      "UnsafeResponsesTotalUsed": 300,
      "JailbreakingResistance": "2.70%",
      "JailbreakingTotalUsed": 37,
      "Latency": "-",
      "GPQA": "84.60%",
      "Multimodal": "Yes",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "No"
    },
    {
      "Model": "Grok-3 Mini",
      "Description": "Grok-3 Mini is xAI's compact yet powerful language model, offering the same advanced capabilities as Grok-3 but with optimized efficiency and lower costs. With multimodal support and strong GPQA performance, it provides excellent value for applications requiring high-quality AI at reduced pricing.",
      "Meta-description": "Grok-3 Mini by xAI - Compact yet powerful AI model with multimodal support and strong GPQA performance. Excellent value with reduced pricing and optimized efficiency.",
      "OperationalRank": "-",
      "SafetyRank": "N/A",
      "Org.": "xAI",
      "Size": "-",
      "Released": "01-May-24",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "$0.30",
      "Output Cost/M": "$0.50",
      "CutoffKnowledge": "2024-11-17",
      "ContextLength": "128,000 tokens",
      "License": "Proprietary",
      "SafeResponses": "-",
      "SafeResponsesTotalUsed": "-",
      "UnsafeResponses": "-",
      "UnsafeResponsesTotalUsed": "-",
      "JailbreakingResistance": "-",
      "JailbreakingTotalUsed": "-",
      "Latency": "-",
      "GPQA": "84.60%",
      "Multimodal": "Yes",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "No"
    },
    {
      "Model": "Grok-2",
      "Description": "Grok-2 is xAI's advanced language model with strong multimodal capabilities and solid performance across benchmarks. With 56% GPQA and 87.5% MMLU scores, it offers reliable AI performance for diverse applications requiring both text and visual understanding.",
      "Meta-description": "Grok-2 by xAI - Advanced multimodal AI with 56% GPQA and 87.5% MMLU performance. Reliable AI for diverse text and visual understanding applications.",
      "OperationalRank": "-",
      "SafetyRank": "N/A",
      "Org.": "xAI",
      "Size": "-",
      "Released": "01-Aug-24",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "$2.00",
      "Output Cost/M": "$10.00",
      "CutoffKnowledge": "2024-08",
      "ContextLength": "128,000 tokens",
      "License": "Proprietary",
      "SafeResponses": "-",
      "SafeResponsesTotalUsed": "-",
      "UnsafeResponses": "-",
      "UnsafeResponsesTotalUsed": "-",
      "JailbreakingResistance": "-",
      "JailbreakingTotalUsed": "-",
      "Latency": "-",
      "GPQA": "56.00%",
      "MMLU": "87.50%",
      "Multimodal": "Yes",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "No"
    },
    {
      "Model": "Grok-2 mini",
      "Description": "Grok-2 mini provides xAI's efficient AI capabilities with multimodal support and competitive benchmark performance. With 51% GPQA and 86.2% MMLU scores, it offers cost-effective AI solutions for applications requiring good performance at optimized pricing.",
      "Meta-description": "Grok-2 mini by xAI - Efficient multimodal AI with 51% GPQA and 86.2% MMLU performance. Cost-effective solution with good performance at optimized pricing.",
      "OperationalRank": "-",
      "SafetyRank": "N/A",
      "Org.": "xAI",
      "Size": "-",
      "Released": "01-Aug-24",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "-",
      "Output Cost/M": "-",
      "CutoffKnowledge": "2024-08",
      "ContextLength": "128,000 tokens",
      "License": "Proprietary",
      "SafeResponses": "-",
      "SafeResponsesTotalUsed": "-",
      "UnsafeResponses": "-",
      "UnsafeResponsesTotalUsed": "-",
      "JailbreakingResistance": "-",
      "JailbreakingTotalUsed": "-",
      "Latency": "-",
      "GPQA": "51.00%",
      "MMLU": "86.20%",
      "Multimodal": "Yes",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "No"
    },
    {
      "Model": "Grok-1.5",
      "Description": "Grok-1.5 is xAI's foundational language model offering solid performance with 35.9% GPQA and 81.3% MMLU scores. As a text-only model, it provides reliable AI capabilities for applications focused on language understanding and generation without multimodal requirements.",
      "Meta-description": "Grok-1.5 by xAI - Foundational text-only AI with 35.9% GPQA and 81.3% MMLU performance. Reliable language understanding and generation capabilities.",
      "OperationalRank": "-",
      "SafetyRank": "N/A",
      "Org.": "xAI",
      "Size": "-",
      "Released": "01-Mar-24",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "-",
      "Output Cost/M": "-",
      "CutoffKnowledge": "-",
      "ContextLength": "128,000 tokens",
      "License": "Proprietary",
      "SafeResponses": "-",
      "SafeResponsesTotalUsed": "-",
      "UnsafeResponses": "-",
      "UnsafeResponsesTotalUsed": "-",
      "JailbreakingResistance": "-",
      "JailbreakingTotalUsed": "-",
      "Latency": "-",
      "GPQA": "35.90%",
      "MMLU": "81.30%",
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "No"
    },
    {
      "Model": "Grok-1.5V",
      "Description": "Grok-1.5V extends xAI's capabilities with multimodal processing, enabling both text and visual understanding. This vision-enabled model provides enhanced AI interactions for applications requiring image analysis and visual reasoning capabilities.",
      "Meta-description": "Grok-1.5V by xAI - Vision-enabled multimodal AI for text and visual understanding. Enhanced interactions with image analysis and visual reasoning capabilities.",
      "OperationalRank": "-",
      "SafetyRank": "N/A",
      "Org.": "xAI",
      "Size": "-",
      "Released": "01-Mar-24",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "-",
      "Output Cost/M": "-",
      "CutoffKnowledge": "-",
      "ContextLength": "128,000 tokens",
      "License": "Proprietary",
      "SafeResponses": "-",
      "SafeResponsesTotalUsed": "-",
      "UnsafeResponses": "-",
      "UnsafeResponsesTotalUsed": "-",
      "JailbreakingResistance": "-",
      "JailbreakingTotalUsed": "-",
      "Latency": "-",
      "GPQA": "-",
      "Multimodal": "Yes",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "No"
    },
    {
      "Model": "Llama 3.1",
      "Description": "Llama 3.1 is Meta's powerful open-source model with 70B parameters, offering strong coding capabilities (1209 CodeLMArena) and mathematical reasoning (34.4% MathLiveBench). With 128K context and competitive pricing, it provides enterprise-level performance with open-source flexibility.",
      "Meta-description": "Llama 3.1 by Meta - Powerful 70B parameter open-source AI with strong coding and math abilities. Enterprise performance with open-source flexibility and competitive pricing.",
      "OperationalRank": "#23",
      "SafetyRank": "N/A",
      "Org.": "Meta",
      "Size": "70B Parameters",
      "Released": "23-Jul-24",
      "CodeLMArena": "1209",
      "MathLiveBench": "34.40%",
      "CodeLiveBench": "-",
      "Input Cost/M": "$1 ",
      "Output Cost/M": "$3 ",
      "CutoffKnowledge": "2023-12-01",
      "ContextLength": "128,000 tokens",
      "License": "Open Source",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "Yes"
    },
    {
      "Model": "Llama 3.2",
      "Description": "Llama 3.2 is Meta's compact yet efficient model with 3B parameters, designed for cost-effective AI applications. Ranked #12 operationally with ultra-low pricing and 128K context, it's perfect for applications requiring good performance at minimal cost.",
      "Meta-description": "Llama 3.2 by Meta - Compact 3B parameter model ranked #12 with ultra-low pricing. Cost-effective AI for applications requiring good performance at minimal cost.",
      "OperationalRank": "#12",
      "SafetyRank": "N/A",
      "Org.": "Meta",
      "Size": "3B Parameters",
      "Released": "25-Sep-24",
      "CodeLMArena": "1048",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "$0.08 ",
      "Output Cost/M": "$0.10 ",
      "CutoffKnowledge": "2023-12-31",
      "ContextLength": "128,000 tokens",
      "License": "Open Source",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "Yes",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "Yes"
    },
    {
      "Model": "Llama 3.2 (Vision)",
      "Description": "Llama 3.2 Vision extends Meta's capabilities with multimodal processing, featuring 90B parameters for both text and visual understanding. This open-source model enables applications requiring image analysis, visual reasoning, and multimodal AI interactions.",
      "Meta-description": "Llama 3.2 Vision by Meta - Multimodal 90B parameter open-source AI with text and visual understanding. Perfect for image analysis and visual reasoning applications.",
      "OperationalRank": "#38",
      "SafetyRank": "N/A",
      "Org.": "Meta",
      "Size": "90B Parameters",
      "Released": "25-Sep-24",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "$0.90 ",
      "Output Cost/M": "$0.90 ",
      "CutoffKnowledge": "2023-12-01",
      "ContextLength": "128,000 tokens",
      "License": "Open Source",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "Yes",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "Yes"
    },
    {
      "Model": "Llama 3.3",
      "Description": "Llama 3.3 represents Meta's enhanced 70B parameter model with improved mathematical reasoning (41.1% MathLiveBench) and coding capabilities. With ultra-competitive pricing and open-source accessibility, it offers excellent value for developers and organizations seeking reliable AI performance.",
      "Meta-description": "Llama 3.3 by Meta - Enhanced 70B parameter model with improved math (41.1%) and coding abilities. Ultra-competitive pricing with open-source accessibility.",
      "OperationalRank": "#19",
      "SafetyRank": "N/A",
      "Org.": "Meta",
      "Size": "70B Parameters",
      "Released": "06-Dec-24",
      "CodeLMArena": "1232",
      "MathLiveBench": "41.10%",
      "CodeLiveBench": "-",
      "Input Cost/M": "$0.10 ",
      "Output Cost/M": "$0.40 ",
      "CutoffKnowledge": "2023-12-01",
      "ContextLength": "-",
      "License": "Open Source",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "Yes"
    },
    {
      "Model": "Llama 4 Behemoth",
      "Description": "Llama 4 Behemoth is Meta's massive 2T parameter open-source model, designed for the most demanding AI applications. With cutting-edge architecture and current knowledge, it represents the pinnacle of open-source AI capability for research and enterprise use.",
      "Meta-description": "Llama 4 Behemoth by Meta - Massive 2T parameter open-source AI model for demanding applications. Pinnacle of open-source AI capability for research and enterprise use.",
      "OperationalRank": "#58",
      "SafetyRank": "N/A",
      "Org.": "Meta",
      "Size": "2T Parameters",
      "Released": "05-Apr-25",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "-",
      "Output Cost/M": "-",
      "CutoffKnowledge": "2024-08-01",
      "ContextLength": "-",
      "License": "Open Source",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "Yes"
    },
    {
      "Model": "Llama 4 Maverick",
      "Description": "Llama 4 Maverick achieves the #1 operational ranking with 400B parameters, representing Meta's most advanced open-source model. With exceptional performance and ultra-competitive pricing, it sets new standards for open-source AI accessibility and capability.",
      "Meta-description": "Llama 4 Maverick by Meta - #1 ranked 400B parameter open-source AI model. Most advanced open-source AI with exceptional performance and ultra-competitive pricing.",
      "OperationalRank": "#1",
      "SafetyRank": "N/A",
      "Org.": "Meta",
      "Size": "400B Parameters",
      "Released": "05-Apr-25",
      "CodeLMArena": "1265",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "$0.19 ",
      "Output Cost/M": "$0.49 ",
      "CutoffKnowledge": "2024-08-01",
      "ContextLength": "1M tokens",
      "License": "Open Source",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "Yes",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "Yes"
    },
    {
      "Model": "Llama 4 Maverick 128e Instruct",
      "Description": "Llama 4 Maverick 17B is Meta's efficient MoE model with 17B active parameters from a 400B total architecture. Ranked #9 in safety with 93% safe responses, it combines high performance with responsible AI deployment and cost efficiency.",
      "Meta-description": "Llama 4 Maverick 17B by Meta - Efficient MoE model with 17B active parameters and #9 safety ranking (93% safe). High performance with responsible AI deployment.",
      "OperationalRank": "#33",
      "SafetyRank": "#11",
      "Org.": "Meta",
      "Size": "17B Parameters (400B total with MoE)",
      "Released": "05-Apr-25",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "$0.20 ",
      "Output Cost/M": "$0.60 ",
      "CutoffKnowledge": "2024-08-01",
      "ContextLength": "128,000 tokens",
      "License": "Open Source",
      "SafeResponses": "93%",
      "SafeResponsesTotalUsed": 300,
      "UnsafeResponses": "6%",
      "UnsafeResponsesTotalUsed": 300,
      "JailbreakingResistance": "1%",
      "JailbreakingTotalUsed": 37,
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "Yes"
    },
    {
      "Model": "Llama 4 Scout",
      "Description": "Llama 4 Scout features 109B parameters with an extraordinary 10M token context window, designed for applications requiring massive context understanding. This open-source model excels at processing large documents, codebases, and extended conversations.",
      "Meta-description": "Llama 4 Scout by Meta - 109B parameter model with extraordinary 10M token context window. Perfect for large document processing and extended conversations.",
      "OperationalRank": "#42",
      "SafetyRank": "N/A",
      "Org.": "Meta",
      "Size": "109B Parameters",
      "Released": "05-Apr-25",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "-",
      "Output Cost/M": "-",
      "CutoffKnowledge": "2024-08-01",
      "ContextLength": "10M tokens",
      "License": "Open Source",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "Yes",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "Yes"
    },
    {
      "Model": "Llama 3.1 Instant",
      "Description": "Llama 3.1 8B Instant is Meta's optimized model for rapid response applications with 8B parameters. Ranked #8 in safety with 94% safe responses, it provides reliable AI capabilities with excellent safety measures for production deployments requiring speed.",
      "Meta-description": "Llama 3.1 8B Instant by Meta - Optimized 8B parameter model for rapid response with #8 safety ranking (94% safe). Reliable AI with excellent safety for speed-critical applications.",
      "OperationalRank": "#64",
      "SafetyRank": "#10",
      "Org.": "Meta",
      "Size": "8B Parameters",
      "Released": "23-Jul-24",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "-",
      "Output Cost/M": "-",
      "CutoffKnowledge": "2023-12-01",
      "ContextLength": "128,000 tokens",
      "License": "Open Source",
      "SafeResponses": "94%",
      "SafeResponsesTotalUsed": 300,
      "UnsafeResponses": "5%",
      "UnsafeResponsesTotalUsed": 300,
      "JailbreakingResistance": "1%",
      "JailbreakingTotalUsed": 37,
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "Yes"
    },
    {
      "Model": "Llama 4 Scout 16e Instruct",
      "Description": "Llama 4 Scout 17B is Meta's specialized MoE instruction model with 17B parameters, designed for efficient task completion. Ranked #10 in safety with 91% safe responses, it balances performance with responsible AI deployment for instruction-following applications.",
      "Meta-description": "Llama 4 Scout 17B by Meta - Specialized MoE instruction model with #10 safety ranking (91% safe). Balanced performance with responsible AI for instruction-following tasks.",
      "OperationalRank": "#52",
      "SafetyRank": "#13",
      "Org.": "Meta",
      "Size": "17B Parameters (MoE)",
      "Released": "05-Apr-25",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "-",
      "Output Cost/M": "-",
      "CutoffKnowledge": "2024-08-01",
      "ContextLength": "128,000 tokens",
      "License": "Open Source",
      "SafeResponses": "91%",
      "SafeResponsesTotalUsed": 300,
      "UnsafeResponses": "8%",
      "UnsafeResponsesTotalUsed": 300,
      "JailbreakingResistance": "1%",
      "JailbreakingTotalUsed": 37,
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "Yes"
    },
    {
      "Model": "Mistral Large",
      "Description": "Mistral Large is a powerful 123B parameter model designed for complex reasoning and professional applications. With strong mathematical capabilities (43.7% MathLiveBench) and coding performance, it offers European AI excellence with competitive pricing for enterprise use.",
      "Meta-description": "Mistral Large - Powerful 123B parameter AI model with strong math (43.7%) and coding performance. European AI excellence with competitive pricing for enterprise applications.",
      "OperationalRank": "#25",
      "SafetyRank": "N/A",
      "Org.": "Mistral",
      "Size": "123B Parameters",
      "Released": "18-Nov-24",
      "CodeLMArena": "1228",
      "MathLiveBench": "32.60%",
      "CodeLiveBench": "-",
      "Input Cost/M": "$2 ",
      "Output Cost/M": "$6 ",
      "CutoffKnowledge": "2024-07-01",
      "ContextLength": "128,000 tokens",
      "License": "Proprietary",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "No"
    },
    {
      "Model": "Mistral Large",
      "Description": "Mistral Large (July 2024 variant) delivers enhanced performance with 123B parameters and improved mathematical reasoning (43.7% MathLiveBench). This European AI solution provides strong coding capabilities and professional-grade performance for demanding applications.",
      "Meta-description": "Mistral Large July 2024 - Enhanced 123B parameter AI with improved math reasoning (43.7%). European AI solution with strong coding capabilities for professional applications.",
      "OperationalRank": "#25",
      "SafetyRank": "N/A",
      "Org.": "Mistral",
      "Size": "123B Parameters",
      "Released": "24-Jul-24",
      "CodeLMArena": "1244",
      "MathLiveBench": "43.70%",
      "CodeLiveBench": "-",
      "Input Cost/M": "$2 ",
      "Output Cost/M": "$6 ",
      "CutoffKnowledge": "2024-07-01",
      "ContextLength": "128,000 tokens",
      "License": "Proprietary",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "No"
    },
    {
      "Model": "Mistral Medium 3",
      "Description": "Mistral Medium 3 offers balanced performance for mid-tier applications requiring reliable AI capabilities. With competitive pricing and European data sovereignty, it's ideal for businesses seeking dependable AI solutions without premium costs.",
      "Meta-description": "Mistral Medium 3 - Balanced AI model for mid-tier applications with competitive pricing. European data sovereignty and dependable performance for business applications.",
      "OperationalRank": "#36",
      "SafetyRank": "N/A",
      "Org.": "Mistral",
      "Size": "-",
      "Released": "07-May-25",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "$0.40 ",
      "Output Cost/M": "$2 ",
      "CutoffKnowledge": null,
      "ContextLength": "-",
      "License": "-",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "No"
    },
    {
      "Model": "Mistral Nemo",
      "Description": "Mistral Nemo is a compact 12B parameter model optimized for efficiency and cost-effectiveness. With mathematical reasoning capabilities (16.9% MathLiveBench) and 128K context, it provides reliable AI performance for applications requiring good capabilities at competitive prices.",
      "Meta-description": "Mistral Nemo - Compact 12B parameter model optimized for efficiency with math capabilities (16.9%). Reliable AI performance at competitive prices for cost-conscious applications.",
      "OperationalRank": "#32",
      "SafetyRank": "N/A",
      "Org.": "Mistral",
      "Size": "12B Parameters",
      "Released": "24-Jul-24",
      "CodeLMArena": "-",
      "MathLiveBench": "16.90%",
      "CodeLiveBench": "-",
      "Input Cost/M": "$0.15 ",
      "Output Cost/M": "$0.15 ",
      "CutoffKnowledge": "2024-04-01",
      "ContextLength": "128,000 tokens",
      "License": "Proprietary",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "No"
    },
    {
      "Model": "Mistral Small 3",
      "Description": "Mistral Small 3 is a lightweight 24B parameter model designed for efficient AI applications. With compact context and European AI heritage, it provides reliable performance for applications requiring good capabilities without extensive resource requirements.",
      "Meta-description": "Mistral Small 3 - Lightweight 24B parameter model for efficient AI applications. European AI heritage with reliable performance for resource-conscious deployments.",
      "OperationalRank": "#57",
      "SafetyRank": "N/A",
      "Org.": "Mistral",
      "Size": "24B Parameters",
      "Released": "30-Jan-25",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "-",
      "Output Cost/M": "-",
      "CutoffKnowledge": null,
      "ContextLength": "32,000 tokens",
      "License": "Proprietary",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "No"
    },
    {
      "Model": "Mistral Small 3.1",
      "Description": "Mistral Small 3.1 enhances the compact model line with 24B parameters and extended 128K context. As an open-source offering, it provides European AI innovation with transparency and customization options for developers and researchers.",
      "Meta-description": "Mistral Small 3.1 - Enhanced 24B parameter open-source model with 128K context. European AI innovation with transparency and customization for developers.",
      "OperationalRank": "#53",
      "SafetyRank": "N/A",
      "Org.": "Mistral",
      "Size": "24B Parameters",
      "Released": "17-Mar-25",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "-",
      "Output Cost/M": "-",
      "CutoffKnowledge": null,
      "ContextLength": "128,000 tokens",
      "License": "Open Source",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "Yes"
    },
    {
      "Model": "Pixtral Large",
      "Description": "Pixtral Large is Mistral's multimodal model with 123B parameters, combining text and visual understanding capabilities. This advanced model enables applications requiring image analysis, visual reasoning, and multimodal AI interactions with European AI standards.",
      "Meta-description": "Pixtral Large by Mistral - Multimodal 123B parameter AI with text and visual understanding. Advanced image analysis and visual reasoning with European AI standards.",
      "OperationalRank": "#47",
      "SafetyRank": "N/A",
      "Org.": "Mistral",
      "Size": "123B Parameters",
      "Released": "18-Nov-24",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "$2 ",
      "Output Cost/M": "$6 ",
      "CutoffKnowledge": null,
      "ContextLength": "128,000 tokens",
      "License": "Proprietary",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "No"
    },
    {
      "Model": "Qwen 2.5",
      "Description": "Qwen 2.5 (7B) is Alibaba's efficient model with strong mathematical reasoning (38.2% MathLiveBench) and current knowledge. With 128K context and open-source accessibility, it provides excellent performance for applications requiring modern AI capabilities at scale.",
      "Meta-description": "Qwen 2.5 7B by Alibaba - Efficient model with strong math reasoning (38.2%) and current knowledge. Open-source accessibility with excellent performance for modern AI applications.",
      "OperationalRank": "#15",
      "SafetyRank": "N/A",
      "Org.": "Alibaba",
      "Size": "7B Parameters",
      "Released": "19-Sep-24",
      "CodeLMArena": "-",
      "MathLiveBench": "38.20%",
      "CodeLiveBench": "-",
      "Input Cost/M": "-",
      "Output Cost/M": "-",
      "CutoffKnowledge": "2024-09-01",
      "ContextLength": "128,000 tokens",
      "License": "Open Source",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "Yes"
    },
    {
      "Model": "Qwen 2.5",
      "Description": "Qwen 2.5 (72B) represents Alibaba's flagship open-source model with exceptional mathematical reasoning (52.4% MathLiveBench) and strong coding capabilities. With competitive pricing and current knowledge, it offers enterprise-level performance with transparency.",
      "Meta-description": "Qwen 2.5 72B by Alibaba - Flagship open-source model with exceptional math (52.4%) and coding capabilities. Enterprise performance with competitive pricing and transparency.",
      "OperationalRank": "#15",
      "SafetyRank": "N/A",
      "Org.": "Alibaba",
      "Size": "72B Parameters",
      "Released": "19-Sep-24",
      "CodeLMArena": "1247",
      "MathLiveBench": "52.40%",
      "CodeLiveBench": "-",
      "Input Cost/M": "$0.38 ",
      "Output Cost/M": "$0.57 ",
      "CutoffKnowledge": "2024-09-01",
      "ContextLength": "128,000 tokens",
      "License": "Open Source",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "Yes"
    },
    {
      "Model": "Qwen 2.5",
      "Description": "Qwen 2.5 (32B) offers balanced performance with 32B parameters, providing strong capabilities for mid-scale applications. With current knowledge and open-source accessibility, it's ideal for organizations seeking reliable AI performance with customization flexibility.",
      "Meta-description": "Qwen 2.5 32B by Alibaba - Balanced 32B parameter model for mid-scale applications. Current knowledge and open-source accessibility with customization flexibility.",
      "OperationalRank": "#15",
      "SafetyRank": "N/A",
      "Org.": "Alibaba",
      "Size": "32B Parameters",
      "Released": "19-Sep-24",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "-",
      "Output Cost/M": "-",
      "CutoffKnowledge": "2024-09-01",
      "ContextLength": "128,000 tokens",
      "License": "Open Source",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "Yes"
    },
    {
      "Model": "Qwen 2.5",
      "Description": "Qwen 2.5 (14B) provides efficient AI capabilities with 14B parameters, designed for applications requiring good performance with moderate resource requirements. With current knowledge and open-source licensing, it offers flexibility for diverse use cases.",
      "Meta-description": "Qwen 2.5 14B by Alibaba - Efficient 14B parameter model for moderate resource requirements. Current knowledge and open-source licensing for diverse applications.",
      "OperationalRank": "#15",
      "SafetyRank": "N/A",
      "Org.": "Alibaba",
      "Size": "14B Parameters",
      "Released": "19-Sep-24",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "-",
      "Output Cost/M": "-",
      "CutoffKnowledge": "2024-09-01",
      "ContextLength": "128,000 tokens",
      "License": "Open Source",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "Yes"
    },
    {
      "Model": "Qwen 2.5",
      "Description": "Qwen 2.5 (1.5B) is Alibaba's compact model designed for edge computing and resource-constrained environments. With 1.5B parameters and current knowledge, it enables AI functionality in lightweight applications while maintaining quality performance.",
      "Meta-description": "Qwen 2.5 1.5B by Alibaba - Compact model for edge computing and resource-constrained environments. Current knowledge with quality performance in lightweight applications.",
      "OperationalRank": "#15",
      "SafetyRank": "N/A",
      "Org.": "Alibaba",
      "Size": "1.5B Parameters",
      "Released": "19-Sep-24",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "-",
      "Output Cost/M": "-",
      "CutoffKnowledge": "2024-09-01",
      "ContextLength": "32,000 tokens",
      "License": "Open Source",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "Yes"
    },
    {
      "Model": "Qwen 2.5",
      "Description": "Qwen 2.5 (3B) offers efficient AI capabilities with 3B parameters, balancing performance and resource efficiency. With current knowledge and open-source accessibility, it's perfect for applications requiring reliable AI functionality without extensive computational requirements.",
      "Meta-description": "Qwen 2.5 3B by Alibaba - Efficient 3B parameter model balancing performance and resource efficiency. Current knowledge and open-source accessibility for reliable AI functionality.",
      "OperationalRank": "#15",
      "SafetyRank": "N/A",
      "Org.": "Alibaba",
      "Size": "3B Parameters",
      "Released": "19-Sep-24",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "-",
      "Output Cost/M": "-",
      "CutoffKnowledge": "2024-09-01",
      "ContextLength": "32,000 tokens",
      "License": "Open Source",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "Yes"
    },
    {
      "Model": "Qwen 2.5 Coder",
      "Description": "Qwen 2.5 Coder (32B) is Alibaba's specialized programming model with strong mathematical reasoning (46% MathLiveBench) and coding capabilities. Designed specifically for software development, it provides excellent programming assistance with current knowledge and competitive pricing.",
      "Meta-description": "Qwen 2.5 Coder 32B by Alibaba - Specialized programming model with strong math (46%) and coding capabilities. Excellent programming assistance with current knowledge.",
      "OperationalRank": "#16",
      "SafetyRank": "N/A",
      "Org.": "Alibaba",
      "Size": "32B Parameters",
      "Released": "12-Nov-24",
      "CodeLMArena": "1227",
      "MathLiveBench": "46.00%",
      "CodeLiveBench": "-",
      "Input Cost/M": "$0.20 ",
      "Output Cost/M": "$0.20 ",
      "CutoffKnowledge": "2024-09-01",
      "ContextLength": "128,000 tokens",
      "License": "Open Source",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "Yes"
    },
    {
      "Model": "Qwen 2.5 Coder",
      "Description": "Qwen 2.5 Coder (14B) provides efficient coding assistance with 14B parameters, optimized for software development tasks. With current knowledge and open-source licensing, it offers reliable programming support for developers and coding teams.",
      "Meta-description": "Qwen 2.5 Coder 14B by Alibaba - Efficient coding assistance with 14B parameters for software development. Current knowledge and open-source licensing for programming support.",
      "OperationalRank": "#16",
      "SafetyRank": "N/A",
      "Org.": "Alibaba",
      "Size": "14B Parameters",
      "Released": "12-Nov-24",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "-",
      "Output Cost/M": "-",
      "CutoffKnowledge": "2024-09-01",
      "ContextLength": "128,000 tokens",
      "License": "Open Source",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "Yes"
    },
    {
      "Model": "Qwen 2.5 Coder",
      "Description": "Qwen 2.5 Coder (7B) offers compact coding assistance with 7B parameters, designed for efficient programming support. With current knowledge and open-source accessibility, it provides reliable coding capabilities for resource-conscious development environments.",
      "Meta-description": "Qwen 2.5 Coder 7B by Alibaba - Compact coding assistance with 7B parameters for efficient programming support. Current knowledge and open-source accessibility.",
      "OperationalRank": "#16",
      "SafetyRank": "N/A",
      "Org.": "Alibaba",
      "Size": "7B Parameters",
      "Released": "12-Nov-24",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "-",
      "Output Cost/M": "-",
      "CutoffKnowledge": "2024-09-01",
      "ContextLength": "128,000 tokens",
      "License": "Open Source",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "Yes"
    },
    {
      "Model": "Qwen 2.5 Coder",
      "OperationalRank": "#16",
      "SafetyRank": "N/A",
      "Org.": "Alibaba",
      "Size": "3B Parameters",
      "Released": "12-Nov-24",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "-",
      "Output Cost/M": "-",
      "CutoffKnowledge": "2024-09-01",
      "ContextLength": "32,000 tokens",
      "License": "Open Source",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "Yes"
    },
    {
      "Model": "Qwen 2.5 Coder",
      "OperationalRank": "#16",
      "SafetyRank": "N/A",
      "Org.": "Alibaba",
      "Size": "1.5B Parameters",
      "Released": "12-Nov-24",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "$0.20 ",
      "Output Cost/M": "$0.60 ",
      "CutoffKnowledge": "2024-09-01",
      "ContextLength": "32,000 tokens",
      "License": "Open Source",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "Yes"
    },
    {
      "Model": "Qwen 2.5 Coder",
      "OperationalRank": "#16",
      "SafetyRank": "N/A",
      "Org.": "Alibaba",
      "Size": "0.5B Parameters",
      "Released": "12-Nov-24",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "-",
      "Output Cost/M": "-",
      "CutoffKnowledge": "2024-09-01",
      "ContextLength": "32,000 tokens",
      "License": "Open Source",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "Yes"
    },
    {
      "Model": "Qwen 2.5 Max",
      "OperationalRank": "#24",
      "SafetyRank": "N/A",
      "Org.": "Alibaba",
      "Size": "-",
      "Released": "28-Jan-25",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "33.80%",
      "Input Cost/M": "$0 ",
      "Output Cost/M": "$0.01 ",
      "CutoffKnowledge": "2024-09-01",
      "ContextLength": "32,000 tokens",
      "License": "Proprietary",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "No"
    },
    {
      "Model": "Qwen 2.5 Turbo",
      "OperationalRank": "#45",
      "SafetyRank": "N/A",
      "Org.": "Alibaba",
      "Size": "-",
      "Released": "15-Nov-24",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "-",
      "Output Cost/M": "-",
      "CutoffKnowledge": "2024-09-01",
      "ContextLength": "1M tokens",
      "License": "Proprietary",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "No"
    },
    {
      "Model": "Qwen 2.5VL",
      "OperationalRank": "#59",
      "SafetyRank": "N/A",
      "Org.": "Alibaba",
      "Size": "32B Parameters",
      "Released": "24-Mar-25",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "-",
      "Output Cost/M": "-",
      "CutoffKnowledge": null,
      "ContextLength": "-",
      "License": "Open Source",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "Yes"
    },
    {
      "Model": "QWen-qwq-32b",
      "OperationalRank": "#66",
      "SafetyRank": "#8",
      "Org.": "Alibaba",
      "Size": "32B Parameters",
      "Released": "12-Oct-24",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "-",
      "Output Cost/M": "-",
      "CutoffKnowledge": "2024-09-01",
      "ContextLength": "128,000 tokens",
      "License": "Open Source",
      "SafeResponses": "94%",
      "SafeResponsesTotalUsed": 300,
      "UnsafeResponses": "5%",
      "UnsafeResponsesTotalUsed": 300,
      "JailbreakingResistance": "12%",
      "JailbreakingTotalUsed": 37,
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "Yes"
    },
    {
      "Model": "R1 Distill Llama",
      "OperationalRank": "#44",
      "SafetyRank": "N/A",
      "Org.": "DeepSeek",
      "Size": "70B Parameters",
      "Released": "20-Jan-25",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "46.60%",
      "Input Cost/M": "-",
      "Output Cost/M": "-",
      "CutoffKnowledge": "2023-12-01",
      "ContextLength": "-",
      "License": "Open Source",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "Yes"
    },
    {
      "Model": "R1 Distill Llama",
      "OperationalRank": "#44",
      "SafetyRank": "N/A",
      "Org.": "DeepSeek",
      "Size": "8B Parameters",
      "Released": "20-Jan-25",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "-",
      "Output Cost/M": "-",
      "CutoffKnowledge": "2024-12-01",
      "ContextLength": "-",
      "License": "Open Source",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "Yes"
    },
    {
      "Model": "R1 Distill Qwen",
      "OperationalRank": "#40",
      "SafetyRank": "N/A",
      "Org.": "DeepSeek",
      "Size": "32B Parameters",
      "Released": "20-Jan-25",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "52.30%",
      "Input Cost/M": "-",
      "Output Cost/M": "-",
      "CutoffKnowledge": "2024-09-01",
      "ContextLength": "-",
      "License": "Open Source",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "Yes"
    },
    {
      "Model": "R1 Distill Qwen",
      "OperationalRank": "#40",
      "SafetyRank": "N/A",
      "Org.": "DeepSeek",
      "Size": "14B Parameters",
      "Released": "20-Jan-25",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "-",
      "Output Cost/M": "-",
      "CutoffKnowledge": "2024-09-01",
      "ContextLength": "-",
      "License": "Open Source",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "Yes"
    },
    {
      "Model": "R1 Distill Qwen",
      "OperationalRank": "#40",
      "SafetyRank": "N/A",
      "Org.": "DeepSeek",
      "Size": "7B Parameters",
      "Released": "20-Jan-25",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "-",
      "Output Cost/M": "-",
      "CutoffKnowledge": "2024-09-01",
      "ContextLength": "-",
      "License": "Open Source",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "Yes"
    },
    {
      "Model": "R1 Distill Qwen",
      "OperationalRank": "#40",
      "SafetyRank": "N/A",
      "Org.": "DeepSeek",
      "Size": "1.5B Parameters",
      "Released": "20-Jan-25",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "-",
      "Output Cost/M": "-",
      "CutoffKnowledge": "2024-09-01",
      "ContextLength": "-",
      "License": "Open Source",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "Yes"
    },
    {
      "Model": "R1 Lite Preview",
      "OperationalRank": "#75",
      "SafetyRank": "N/A",
      "Org.": "DeepSeek",
      "Size": "-",
      "Released": "20-Nov-24",
      "CodeLMArena": "-",
      "MathLiveBench": "-",
      "CodeLiveBench": "-",
      "Input Cost/M": "-",
      "Output Cost/M": "-",
      "CutoffKnowledge": null,
      "ContextLength": "-",
      "License": "Proprietary",
      "SafeResponses": "N/A",
      "UnsafeResponses": "N/A",
      "JailbreakingResistance": "N/A",
      "Latency": "-",
      "GPQA": null,
      "Multimodal": "No",
      "Reasoning": "No",
      "Web Access": "No",
      "Fine-tunable": "No"
    },
    {
      "Model": "Claude 4 Sonnet",
      "Description": "Claude 4 Sonnet is a significant upgrade to Claude Sonnet 3.7, delivering superior coding and reasoning while responding more precisely to instructions. Leading on SWE-bench with 72.7% performance, it balances high performance with efficiency for both internal and external use cases.",
      "Meta-description": "Claude 4 Sonnet by Anthropic - Superior coding and reasoning upgrade to Sonnet 3.7. Leading SWE-bench performance (72.7%) with enhanced instruction following and efficiency.",
      "OperationalRank": "#5",
      "SafetyRank": "#3",
      "Org.": "Anthropic",
      "Size": "-",
      "Released": "22-May-25",
      "CodeLMArena": "-",
      "MathLiveBench": "70.5%",
      "CodeLiveBench": "72.7%",
      "Input Cost/M": "$3.00",
      "Output Cost/M": "$15.00",
      "CutoffKnowledge": "2024-04-01",
      "ContextLength": "200,000 tokens",
      "License": "Proprietary",
      "SafeResponses": "98%",
      "SafeResponsesTotalUsed": 99,
      "UnsafeResponses": "1%",
      "UnsafeResponsesTotalUsed": 99,
      "JailbreakingResistance": "98%",
      "JailbreakingTotalUsed": 100,
      "Latency": "~0.5-1s",
      "GPQA": "75.40%",
      "Multimodal": "Yes",
      "Reasoning": "Yes",
      "Web Access": "Yes",
      "Fine-tunable": "No"
    }
  ]